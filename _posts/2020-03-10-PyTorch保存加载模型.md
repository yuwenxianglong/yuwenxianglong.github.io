---
title: PyTorch保存加载模型
author: 赵旭山
tags: PyTorch
---

#### 1. 简介

断点续算，是一个永恒的话题，也是老生常谈的问题。最近一次，是我的领导兼师长带着批评意味跟我谈论我们的学科专业软件VASP，从他的言谈话语中，我虽不满他侵犯我的专业领域，但也隐约感觉到了一些新的讯息，适应当前可依赖的资源，而不是环境可以去适应你。对于此，我非常感激！

于此，正在学习PyTorch的点点滴滴，自然而然的也乍现了这个问题。闲言少叙，继续记录我的学习笔记：PyTorch的保存加载模型，只为断点续算！

#### 2. PyTorch相关命令

`torch.save`

`torch.load`

`torch.nn.Module.load_state_dict`

前两个自然是成对的，我最初的理解就是全部保存和加载，实际使用中并不是那么回事儿，直接`load`会报错，要求先定义`model`的`class`，看样子模型结构应该是没有的。

先照搬过来：

```
"1.torch.save：将序列化的对象保存到disk。这个函数使用Python的pickle实用程序进行序列化。使用这个函数可以保存各种对象的模型、张量和字典。"

"2.torch.load：使用pickle unpickle工具将pickle的对象文件反序列化为内存。"

"3.torch.nn.Module.load_state_dict:使用反序列化状态字典加载model’s参数字典。"
```

至于啥是序列化，反正我现在是没有完全理解，就是一个序列吧，按`nn.Sequential`理解的。

就一个模型而言，须保存的，结构、参数、权重、误差，是最必要的。

#### 2. 先说“`state_dict`”

感觉上，这个应该不如`torch.save`，没`torch.save`保存的信息全。

`torch.state_dict`是一个python字典对象，将每一层与他的对应参数建立映射关系，例如每一层的权重、偏置量等。

> “注意,只有那些参数可以训练的layer才会被保存到模型的state_dict中,如卷积层,线性层等等”。

优化器`optim`也有一个`state_dict`，其中包括优化器状态及所使用的超参数。

##### 2.1 `model.state_dict`

以[前文](https://yuwenxianglong.github.io/2020/03/09/PyTorch%E6%95%B0%E6%8D%AE%E5%88%86%E7%B1%BB.html)Iris分类问题定义的模型为例：

```python
In [2]: model.state_dict
Out[2]:
<bound method Module.state_dict of Net(
  (fc): Sequential(
    (0): Linear(in_features=4, out_features=8, bias=True)
    (1): ReLU()
    (2): Linear(in_features=8, out_features=3, bias=True)
  )
)>

In [3]: model.state_dict()
Out[3]:
OrderedDict([('fc.0.weight',
              tensor([[-0.2258, -0.2180,  0.4006, -0.2800],
                      [ 0.0815, -0.3166,  1.2569,  2.0008],
                      [ 0.0072, -0.3069,  0.0364, -0.4576],
                      [ 0.4951, -0.1889,  0.8546,  0.2928],
                      [ 0.9734,  1.2420, -1.4061, -2.2928],
                      [ 0.1066, -0.3670,  0.1094,  0.4016],
                      [ 0.7466,  1.0149, -1.4919, -1.2428],
                      [-0.2539,  0.1516,  0.0260, -0.4365]], device='cuda:0')),
             ('fc.0.bias',
              tensor([-0.2878, -3.0231,  0.1572,  1.1301,  2.6686, -0.5526,  0.5653, -0.4981],
                     device='cuda:0')),
             ('fc.2.weight',
              tensor([[-2.4641e-01, -6.2554e-01,  1.9342e-01, -2.2077e-01,  1.0629e+00,
                       -3.3504e-02,  1.1767e+00,  1.0436e-02],
                      [-8.1625e-02, -7.6163e-01,  1.3912e-01,  6.2912e-01,  1.4385e+00,
                       -5.3620e-01, -1.8019e+00,  1.4382e-01],
                      [-2.0634e-01,  6.9011e-01, -3.1452e-01,  3.3671e-01, -1.8798e+00,
                        1.3456e-03, -4.9409e-01, -1.8715e-01]], device='cuda:0')),
             ('fc.2.bias',
              tensor([-0.1675,  0.8099, -0.6197], device='cuda:0'))])
```

`model.state_dict()`比不带“括号”的`model.state_dict`打印了更详细的信息，列出了每一层的具体参数、计算设备等。

##### 2.2 `*optim.state_dict`

```python
In [4]: optimizer.state_dict
Out[4]:
<bound method Optimizer.state_dict of Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.005
    weight_decay: 0
)>

In [5]: optimizer.state_dict()
Out[5]:
{'state': {2744643462632: {'step': 1000,
   'exp_avg': tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
           [-6.3869e-05,  7.6315e-05, -8.3155e-04, -5.7846e-04],
           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
           [-2.1376e-04,  1.2902e-04, -5.0888e-04, -1.7145e-04],
           [ 2.0039e-05, -1.1037e-04,  1.4508e-03,  1.1039e-03],
           [-7.4375e-05, -1.9088e-05, -3.4329e-04, -2.3291e-04],
           [-2.3138e-04, -9.8524e-04,  1.5628e-03,  7.4852e-04],
           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]], device='cuda:0'),
   'exp_avg_sq': tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
           [3.2876e-05, 3.0459e-05, 2.9523e-04, 6.1978e-05],
           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
           [8.0519e-05, 1.9699e-05, 1.1234e-04, 1.5712e-05],
           [3.4613e-04, 9.5902e-05, 3.8095e-04, 1.5965e-04],
           [4.4827e-03, 6.3647e-04, 4.5932e-03, 6.3053e-04],
           [8.8764e-05, 4.0387e-04, 7.8571e-04, 1.3996e-04],
           [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0')},
  2744643462872: {'step': 1000,
   'exp_avg': tensor([ 0.0000,  0.0010,  0.0000, -0.0002, -0.0024,  0.0004, -0.0002,  0.0000],
          device='cuda:0'),
   'exp_avg_sq': tensor([0.0000e+00, 3.0093e-06, 0.0000e+00, 1.5977e-06, 3.2102e-05, 9.2013e-05,
           1.3887e-05, 0.0000e+00], device='cuda:0')},
  2744643369704: {'step': 1000,
   'exp_avg': tensor([[ 0.0000e+00,  1.3745e-03,  0.0000e+00,  1.0239e-03, -1.8121e-03,
             1.2114e-05, -1.5263e-03,  0.0000e+00],
           [ 0.0000e+00,  2.4466e-03,  0.0000e+00, -9.5112e-04, -1.7034e-03,
             6.2730e-04,  1.5204e-03,  0.0000e+00],
           [ 0.0000e+00, -3.8211e-03,  0.0000e+00, -7.2817e-05,  3.5155e-03,
            -6.3942e-04,  5.9241e-06,  0.0000e+00]], device='cuda:0'),
   'exp_avg_sq': tensor([[0.0000, 0.0047, 0.0000, 0.0026, 0.0004, 0.0087, 0.0016, 0.0000],
           [0.0000, 0.0004, 0.0000, 0.0002, 0.0004, 0.0005, 0.0002, 0.0000],
           [0.0000, 0.0048, 0.0000, 0.0016, 0.0010, 0.0065, 0.0009, 0.0000]],
          device='cuda:0')},
  2744643757944: {'step': 1000,
   'exp_avg': tensor([ 1.0168e-05, -6.9278e-04,  6.8261e-04], device='cuda:0'),
   'exp_avg_sq': tensor([3.8737e-04, 7.9728e-05, 3.2682e-04], device='cuda:0')}},
 'param_groups': [{'lr': 0.005,
   'betas': (0.9, 0.999),
   'eps': 1e-08,
   'weight_decay': 0,
   'amsgrad': False,
   'params': [2744643462632, 2744643462872, 2744643369704, 2744643757944]}]}
```

同`model.state_dict`，带括号的列出了更多优化器细节参数。

#### 3. 模型的保存和重载

##### 3.1 仅保存学习到的参数

```python
torch.save(model.state_dict(), *PATH)
```

加载：

```python
model = yourModelClass(*args, **kwargs)  # Model class must be defined somewhere
model.load_state_dict(torch.load(*PATH))
model.eval()
```

> “记住，您必须调用model.eval()，以便在运行推断之前将dropout和batch规范化层设置为评估模式。如果不这样做，将会产生不一致的推断结果。”

##### 3.2 保存整个模型

```python
torch.save(model, *PATH)
```

加载：

```python
model = yourModelClass(*args, **kwargs)  # Model class must be defined somewhere
model = torch.load(torch.load(*PATH))
model.eval()
```

#### 4. 一个复杂点的例子

Save：

```python
       torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        ...
        }, *PATH)
```

Load：

```python
        model = TheModelClass(*args, **kwargs)
        optimizer = TheOptimizerClass(*args, **kwargs)

        checkpoint = torch.load(PATH)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        epoch = checkpoint['epoch']
        loss = checkpoint['loss']

        model.eval()
        # - or -
        model.train()
```









#### References

[PyTorch之保存加载模型](https://www.jianshu.com/p/4905bf8e06e5)

[pytorch 状态字典:state_dict](https://blog.csdn.net/Strive_For_Future/article/details/83240081)

