---
title: 深度学习优化算法
author: 赵旭山
tags: PyTorch
---

#### 1. 概览

对机器学习的算法一知半解，本文是个人总结整理备忘。引用两张[文献](https://arxiv.org/pdf/1609.04747.pdf)中的动态图：

![](/assets/images/optimizationAlgorithmOfDNN.gif)

![](/assets/images/lossSurfaceOfDNN.gif)

> "最优化问题是计算数学中最为重要的研究方向之一。而在深度学习领域，优化算法的选择也是一个模型的重中之重。即使在数据集和模型架构完全相同的情况下，采用不同的优化算法，也很可能导致截然不同的训练效果。
>
> 梯度下降是目前神经网络中使用最为广泛的优化算法之一。为了弥补朴素梯度下降的种种缺陷，研究者们发明了一系列变种算法，从最初的 SGD (随机梯度下降) 逐步演进到 NAdam。然而，许多学术界最为前沿的文章中，都并没有一味使用 Adam/NAdam  等公认“好用”的自适应算法，很多甚至还选择了最为初级的 SGD 或者 SGD with Momentum 等。"

#### 2. 梯度下降（Gradient Descent，GD）

梯度下降算法通过沿梯度的相反方向更新模型参数，学习率𝜂为每一时刻的更新步长。[此文](https://zhuanlan.zhihu.com/p/32626442)给出了梯度下降的流程：

（1） 计算目标函数关于参数的梯度

$$ g_t = \nabla_\theta J(\theta) $$

（2） 根据**历史**梯度计算一阶和二阶动量

$$ m_t = \phi(g_1, g_2, ..., g_t) $$

$$ \upsilon_t = \psi(g_1, g_2, ..., g_t) $$

（3） 更新模型参数

$$ \theta_{t+1} = \theta_t - \frac{1}{\sqrt{\upsilon_t + \epsilon}}m_t $$

其中，$ \epsilon $为平滑项，防止分母为零，通常取$ 10^{-8} $。

#### 2. Gradient Descent变种算法

梯度下降最常见的三种变形：Batch Gradient Descent（BGD）、Stochastic Gradient Descent（SGD）、Mini-Batch Gradient Descent（MBGD），这三种形式的区别就是用多少数据来计算目标函数的梯度。[参阅此文](https://www.cnblogs.com/guoyaohua/p/8542554.html)。

##### 2.1 随机梯度下降（Stochastic Gradient Descent，SGD）









#### References

* [从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)
* [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270)
* [深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)
* [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
* [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/) 
* [Pytorch中常用的四种优化器SGD、Momentum、RMSProp、Adam](https://cloud.tencent.com/developer/article/1491393)
* [深度学习（九） 深度学习最全优化方法总结比较（SGD，Momentum，Nesterov Momentum，Adagrad，Adadelta，RMSprop，Adam）](https://www.bbsmax.com/A/A7zgplBkJ4/)

