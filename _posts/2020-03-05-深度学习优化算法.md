---
title: 深度学习优化算法
author: 赵旭山
tags: 随笔
typora-root-url: ..
---

本文主要参考：

[https://zhuanlan.zhihu.com/p/32626442](https://zhuanlan.zhihu.com/p/32626442)

[https://zhuanlan.zhihu.com/p/22252270](https://zhuanlan.zhihu.com/p/22252270)

#### 1. 概览

对机器学习的算法一知半解，本文是个人总结整理备忘。引用两张[文献](https://arxiv.org/pdf/1609.04747.pdf)中的动态图：

![](/assets/images/optimizationAlgorithmOfDNN.gif)

![](/assets/images/lossSurfaceOfDNN.gif)

> "最优化问题是计算数学中最为重要的研究方向之一。而在深度学习领域，优化算法的选择也是一个模型的重中之重。即使在数据集和模型架构完全相同的情况下，采用不同的优化算法，也很可能导致截然不同的训练效果。
>
> 梯度下降是目前神经网络中使用最为广泛的优化算法之一。为了弥补朴素梯度下降的种种缺陷，研究者们发明了一系列变种算法，从最初的 SGD (随机梯度下降) 逐步演进到 NAdam。然而，许多学术界最为前沿的文章中，都并没有一味使用 Adam/NAdam  等公认“好用”的自适应算法，很多甚至还选择了最为初级的 SGD 或者 SGD with Momentum 等。"

#### 2. 梯度下降（Gradient Descent，GD）

梯度下降算法通过沿梯度的相反方向更新模型参数，学习率$ \eta $为每一时刻的更新步长。[此文](https://zhuanlan.zhihu.com/p/32626442)给出了梯度下降的流程：

（1） 计算目标函数关于参数的梯度

$$ g_t = \nabla_\theta J(\theta) $$

（2） 根据**历史**梯度计算一阶和二阶动量

$$ m_t = \phi(g_1, g_2, ..., g_t) $$

$$ \upsilon_t = \psi(g_1, g_2, ..., g_t) $$

（3） 更新模型参数

$$ \theta_{t+1} = \theta_t - \frac{1}{\sqrt{\upsilon_t + \epsilon}}m_t $$

其中，$ \epsilon $为平滑项，防止分母为零，通常取$ 10^{-8} $。

#### 2. Gradient Descent变种算法

梯度下降最常见的三种变形：Batch Gradient Descent（BGD）、Stochastic Gradient Descent（SGD）、Mini-Batch Gradient Descent（MBGD），这三种形式的区别就是用多少数据来计算目标函数的梯度。

最常用的为：**随机梯度下降（Stochastic Gradient Descent，SGD）**，SGD的数学形式如下：

$$ m_t = \eta g_t $$

$$ \upsilon_t = I^2 $$

$$ \epsilon = 0 $$

更新步骤为：

$$ \theta_{i+1} = \theta_t - \eta g_t $$

SGD的缺点在于收敛速度慢，可能会在鞍点处震荡。并且，如何合理地选择学习率是SGD的一大难点。

#### 3. Momentum

SGD容易陷入局部最优的沟壑中震荡，故引入动量Momentum加速SGD在正确的方向上下降并抑制震荡。

$$ m_t = \gamma m_{t-1} + \eta g_t $$

Momentum（SGD + momentum）即在原步长的基础上，加上了与上一时刻步长相关的$ \gamma m_{t-1} $，$ \gamma $取0.9左右。

> 这意味着参数更新方向不仅由当前的梯度决定，也与此前累积的下降方向有关。这使得参数中那些梯度方向变化不大的维度可以加速更新，并减少梯度方向变化较大的维度上的更新幅度。由此产生了加速收敛和减小震荡的效果。

#### 4. Nesterov Accelerated Gradient (NAG)

NAG算法在目标函数有增高趋势之前，减缓更新速率，从而使下降的过程更加智能。

$$ g_t = \nabla_{\theta} J (\theta - \gamma m_{t-1}) $$

![](/assets/images/NAGUpdate202003061737.jpg)

NAG算法流程描述如下：

* 首先基于Momentum计算一个梯度（短的蓝色向量），然后在加速更新梯度的方向进行一个大的跳跃（长的蓝色向量）；
* 计算出下一时刻$ \theta $的近似位置（棕向量），并根据该未来位置计算梯度（红向量），然后使用和Momentum中相同的方式计算步长（绿向量）；
* 这种计算梯度的方式可以使算法更好地**预测未来**，提前调整更新速率。

NAG在Momentum的基础上进一步改进了梯度计算公式：$ g_t = \nabla_{\theta}J(\theta - \gamma m_{t-1}) $

$$ m_t = \gamma m_{t-1} + g_t $$

$$ \theta_t = \theta_{t-1} - \eta m_t $$

#### 5. Adagrad

SGD、Momentum和NAG算法均以相同的学习率去更新$ \theta $的各个分量，而深度学习模型中往往涉及大量参数，不同参数更新频率是不同的。而Adagrad其实就是对学习率有一个约束，对于更新不频繁的参数，需要步长大一些，多学习一些知识；对于频繁更新的参数，步长需小一些，使参数训练过程更稳定。

Adagrad算法引入了二阶张量：

$$ \upsilon _t = \text{diag}(\sum_{i=1}^t g_{i,1}^2, \sum_{i=1}^t g_{i,2}^2, ..., \sum_{i=1}^t g_{i,d}^2) $$

其中，$ \upsilon_t \in \mathbb{R}^{d \times d} $是对角矩阵，其元素$ \upsilon_{t,ii} $为参数第$ i $维从初始时刻到时刻$ t $的梯度平方和。

> 此时，可以这样理解：学习率等效为$ \frac{\eta}{\sqrt{\upsilon_t + \epsilon}} $ 。对于此前频繁更新过的参数，其二阶动量的对应分量较大，学习率就较小。这一方法在稀疏数据的场景下表现很好。

$$ \upsilon_t = \upsilon_{t-1} + g_{t}^2 $$

$$ \Delta \theta_t = -\frac{\eta}{\sqrt{\upsilon_t + \epsilon}}g_t $$

对$ g_t $从1到$ t $进行一个递推形成一个约束项，$ -\frac{1}{\sqrt{\sum_{i=1}^t(g_i)^2 + \epsilon}} $，$ \epsilon $用来保证分母非0。

***

**特点**：

（1） 前期$ g_t $较小的时候，较$ -\frac{1}{\sqrt{\sum_{i=1}^t(g_i)^2 + \epsilon}} $大，能够放大梯度；

（2） 后期$ g_t $较大的时候，较$ -\frac{1}{\sqrt{\sum_{i=1}^t(g_i)^2 + \epsilon}} $小，能够约束梯度；

（3） 适合处理稀疏梯度。

***

**缺点**：

（1） 由公式可以看出，仍依赖于人工设置一个全局学习率$ \eta $；

（2） $ \eta $设置过大，会使$ -\frac{1}{\sqrt{\sum_{i=1}^t(g_i)^2 + \epsilon}} $过于敏感，对梯度的调节太大；

（3） 中后期，分母上梯度平方的累加将会越来越大，使$ \text{gradient} \rightarrow 0 $，使得训练提前结束。

***

#### 6. Adadelta

Adadelta是对Adagrad的扩展，仍是对学习率进行自适应约束。但Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项。记$ g_t \cdot g_t $为$ g_{t}^2 $，有：

$$ \upsilon_t = \beta \upsilon_{t-1} + (1-\beta)g_{t}^2 $$

$$ \Delta \theta_t = -\frac{\eta}{\sqrt{\upsilon_t + \epsilon}} g_t $$

此处Adadelta还是依赖全局学习率的，但经过了一定处理，经过近似牛顿迭代法之后：

$$ E \lvert g^2 \rvert_t = \rho E \lvert g^2 \rvert_{t-1} + (1-\rho) g_{t}^2 $$

$$ \Delta x_t = -\frac{\sqrt{\sum_{i=1}^{t-1} \Delta x_i}}{\sqrt{E \lvert g^2 \rvert_t + \epsilon}} $$

其中，$ E $代表求期望值。此时，Adadelta已经不依赖于全局学习率了。

**特点**：

（1） 训练初中期，加速效果不错，很快；

（2） 训练后期，反复在局部最小值附近抖动。

#### 7. RMSprop

RMSprop为Adadelta的一个特例：

当$ \rho = 0.5 $时，$ E\lvert g^2 \rvert_t = \rho E \lvert g^2 \rvert_{t-1} + (1 - \rho) g_{t}^2 $就变成了求梯度平方和的平均数。

再求根，就变成了RMS（均方根）：

$$ RMS \lvert g \rvert_t = \sqrt{E \lvert g^2 \rvert_t + \epsilon} $$

这个RMS就可以作为学习率$ \eta $的一个约束：

$$ \Delta x_t = -\frac{\eta}{RMS \lvert g \rvert_t} g_t $$

**特点**：

（1） RMSprop依然依赖于全局学习率；

（2） RMSprop是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间；

（3） 适合处理非平稳目标，对于RNN的效果很好。

#### 8. Adam

Adam可以认为是RMSprop和Momentum的结合，本质上是带有动量项的RMSprop。和RMSprop对二阶动量使用指数移动平均类似，Adam中对一阶动量也是用指数移动平均计算。

$$ m_t = \eta [\beta_1 m_{t-1} + (1 - \beta_1) g_t] $$

$$ \upsilon_t = \beta_2 \upsilon_{t-1} + (1-\beta_2) \cdot \text{diag} (g_{t}^2) $$

其中，$ m_t $和$ \upsilon_t $分别是对梯度的一阶和二阶动量，初值$ m_0 = 0 $，$ \upsilon_0 = 0 $，可以看作对期望$ E \lvert g_t \rvert $和$ E \lvert g_{t}^2 \rvert $的估计。

在迭代初始阶段，$ m_t $和$ \upsilon_t $有一个项初值的偏移（过多的偏向了0）。因此，可以对一阶和二阶动量做偏置校正（bias correction）。

$$ \hat{m_t} = \frac{m_t}{1-\beta_{1}^t} $$

$$ \hat{\upsilon_t} = \frac{\upsilon_t}{1-\beta_{2}^{t}} $$

$ \hat{m_t} $和$ \hat{\upsilon_t} $是对$ m_t $和$ \upsilon_t $的校正，可近似于对期望的无偏差估计。

$$ \theta_{t+1} = \theta_t - \frac{\hat{m_t}}{\sqrt{\hat{\upsilon_t}} + \epsilon} \eta $$

$ -\frac{\hat{m_t}}{\sqrt{\hat{\upsilon_t}} + \epsilon} $对学习率形成了一个动态约束，可以保证迭代较为平稳。

**特点**：

（1） 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点；

（2） 对内存需求较小；

（3） 为不同的参数计算不同的自适应学习率；

（4） 也适用于大多非凸优化，适用于大数据集和高维空间。

#### 9. NAdam

NAdam在Adam之上融合了NAG思想。

NAG的核心在于，计算梯度时使用了“**未来位置**”$$ \theta_t - \gamma m_{t-1} $$。NAdam中提出了一种公式变形，大意可理解为：

> 只要能在梯度计算中考虑到“**未来因素**”，即能达到Nesterov的效果；既然如此，那么在计算梯度时，可以仍然使用原始公式$ g_t = \Delta_{\theta} J(\theta_t) $，但在前一次迭代计算$ \theta_t $时，就使用了未来时刻的动量，即$ \theta_t = \theta_{t-1} - m_t $，那么理论上所达到的效果是类似的。

这时，公式修改为：

$$ g_t = \Delta_{\theta} J(\theta_t) $$

$$ m_t = \gamma m_{t-1} + \eta g_t $$

$$ \overline{m_t} = \gamma m_t + \eta g_t $$

$$ \theta_{t+1} = \theta_t - \overline{m_t} $$

理论上，下一刻动量为$ m_{t+1} = \gamma m_t + \eta g_{t+1} $，在假定连续两次的梯度变化不大的情况下，即$ g_{t+1} \approx g_t $，有$ m_{t+1} \approx \gamma m_t + \eta g_t \equiv \overline{m_t} $。此时，即可用$ \overline{m_t} $近似表示未来动量加入到$ \theta $的迭代式中。

类似的，在Adam可以加入$ \overline{m_t} \leftarrow \hat{m_t} $的变形，将$ \hat{m_t} $展开有：

$$ \hat{m_t} = \frac{m_t}{1-\beta_{1}^t} = \eta [\frac{\beta_1 m_{t-1}}{1 - \beta_{1}^t} + \frac{(1-\beta_1) g_t}{1-\beta_{1}^t}] $$

引入

$$ \overline{m_t} = \eta [\frac{\beta_1 m_t}{1-\beta_{1}^{t+1}} + \frac{(1-\beta_1) g_t}{1-\beta_{1}^t}] $$

再进行更新，

$$ \theta_{t+1} = \theta_t - \frac{1}{\sqrt{\hat{\upsilon_t}} + \epsilon} \overline{m_t} $$

即可在Adam中引入Nesterov加速效果。

可以看出，Nadam对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果。 

#### 10. 小结

* 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值；

* SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠；

* 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法；

* Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多；

* 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果。







#### References

* [从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)
* [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270)
* [深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)
* [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
* [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/) 
* [Pytorch中常用的四种优化器SGD、Momentum、RMSProp、Adam](https://cloud.tencent.com/developer/article/1491393)
* [深度学习（九） 深度学习最全优化方法总结比较（SGD，Momentum，Nesterov Momentum，Adagrad，Adadelta，RMSprop，Adam）](https://www.bbsmax.com/A/A7zgplBkJ4/)

