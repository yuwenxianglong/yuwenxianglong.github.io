---
title: 深度学习优化算法
author: 赵旭山
tags: PyTorch
---

本文主要参考：

[https://zhuanlan.zhihu.com/p/32626442](https://zhuanlan.zhihu.com/p/32626442)

[https://zhuanlan.zhihu.com/p/22252270](https://zhuanlan.zhihu.com/p/22252270)

#### 1. 概览

对机器学习的算法一知半解，本文是个人总结整理备忘。引用两张[文献](https://arxiv.org/pdf/1609.04747.pdf)中的动态图：

![](/assets/images/optimizationAlgorithmOfDNN.gif)

![](/assets/images/lossSurfaceOfDNN.gif)

> "最优化问题是计算数学中最为重要的研究方向之一。而在深度学习领域，优化算法的选择也是一个模型的重中之重。即使在数据集和模型架构完全相同的情况下，采用不同的优化算法，也很可能导致截然不同的训练效果。
>
> 梯度下降是目前神经网络中使用最为广泛的优化算法之一。为了弥补朴素梯度下降的种种缺陷，研究者们发明了一系列变种算法，从最初的 SGD (随机梯度下降) 逐步演进到 NAdam。然而，许多学术界最为前沿的文章中，都并没有一味使用 Adam/NAdam  等公认“好用”的自适应算法，很多甚至还选择了最为初级的 SGD 或者 SGD with Momentum 等。"

#### 2. 梯度下降（Gradient Descent，GD）

梯度下降算法通过沿梯度的相反方向更新模型参数，学习率$ \eta $为每一时刻的更新步长。[此文](https://zhuanlan.zhihu.com/p/32626442)给出了梯度下降的流程：

（1） 计算目标函数关于参数的梯度

$$ g_t = \nabla_\theta J(\theta) $$

（2） 根据**历史**梯度计算一阶和二阶动量

$$ m_t = \phi(g_1, g_2, ..., g_t) $$

$$ \upsilon_t = \psi(g_1, g_2, ..., g_t) $$

（3） 更新模型参数

$$ \theta_{t+1} = \theta_t - \frac{1}{\sqrt{\upsilon_t + \epsilon}}m_t $$

其中，$ \epsilon $为平滑项，防止分母为零，通常取$ 10^{-8} $。

#### 2. Gradient Descent变种算法

梯度下降最常见的三种变形：Batch Gradient Descent（BGD）、Stochastic Gradient Descent（SGD）、Mini-Batch Gradient Descent（MBGD），这三种形式的区别就是用多少数据来计算目标函数的梯度。

最常用的为：**随机梯度下降（Stochastic Gradient Descent，SGD）**，SGD的数学形式如下：

$$ m_t = \eta g_t $$

$$ \upsilon_t = I^2 $$

$$ \epsilon = 0 $$

更新步骤为：

$$ \theta_{i+1} = \theta_t - \eta g_t $$

SGD的缺点在于收敛速度慢，可能会在鞍点处震荡。并且，如何合理地选择学习率是SGD的一大难点。

#### 3. Momentum

SGD容易陷入局部最优的沟壑中震荡，故引入动量Momentum加速SGD在正确的方向上下降并抑制震荡。

$$ m_t = \gamma m_{t-1} + \eta g_t $$

Momentum（SGD + momentum）即在原步长的基础上，加上了与上一时刻步长相关的$ \gamma m_{t-1} $，$ \gamma $取0.9左右。

> 这意味着参数更新方向不仅由当前的梯度决定，也与此前累积的下降方向有关。这使得参数中那些梯度方向变化不大的维度可以加速更新，并减少梯度方向变化较大的维度上的更新幅度。由此产生了加速收敛和减小震荡的效果。

#### 4. Nesterov Accelerated Gradient (NAG)

NAG算法在目标函数有增高趋势之前，减缓更新速率，从而使下降的过程更加智能。

$$ g_t = \nabla_{\theta} J (\theta - \gamma m_{t-1}) $$

![](/assets/images/NAGUpdate202003061737.jpg)

NAG算法流程描述如下：

* 首先基于Momentum计算一个梯度（短的蓝色向量），然后在加速更新梯度的方向进行一个大的跳跃（长的蓝色向量）；
* 计算出下一时刻$ \theta $的近似位置（棕向量），并根据该未来位置计算梯度（红向量），然后使用和Momentum中相同的方式计算步长（绿向量）；
* 这种计算梯度的方式可以使算法更好地**预测未来**，提前调整更新速率。

NAG在Momentum的基础上进一步改进了梯度计算公式：$ g_t = \nabla_{\theta}J(\theta - \gamma m_{t-1}) $

$$ m_t = \gamma m_{t-1} + g_t $$

$$ \theta_t = \theta_{t-1} - \eta m_t $$

#### 5. Adagrad

SGD、Momentum和NAG算法均以相同的学习率去更新$ \theta $的各个分量，而深度学习模型中往往涉及大量参数，不同参数更新频率是不同的。而Adagrad其实就是对学习率有一个约束，对于更新不频繁的参数，需要步长大一些，多学习一些知识；对于频繁更新的参数，步长需小一些，使参数训练过程更稳定。

Adagrad算法引入了二阶张量：

$$ \upsilon _t = \text{diag}(\sum_{i=1}^t g_{i,1}^2, \sum_{i=1}^t g_{i,2}^2, ..., \sum_{i=1}^t g_{i,d}^2) $$

其中，$ \upsilon_t \in \mathbb{R}^{d \times d} $是对角矩阵，其元素$ \upsilon_{t,ii} $为参数第$ i $维从初始时刻到时刻$ t $的梯度平方和。

> 此时，可以这样理解：学习率等效为$ \frac{\eta}{\sqrt{\upsilon_t + \epsilon}} $ 。对于此前频繁更新过的参数，其二阶动量的对应分量较大，学习率就较小。这一方法在稀疏数据的场景下表现很好。

$$ \upsilon_t = \upsilon_{t-1} + g_{t}^2 $$

$$ \Delta \theta_t = -\frac{\eta}{\sqrt{\upsilon_t + \epsilon}}g_t $$

对$ g_t $从1到$ t $进行一个递推形成一个约束项，$ -\frac{1}{\sqrt{\sum_{i=1}^t(g_i)^2 + \epsilon}} $，$ \epsilon $用来保证分母非0。

***

**特点**：

（1） 前期$ g_t $较小的时候，较$ -\frac{1}{\sqrt{\sum_{i=1}^t(g_i)^2 + \epsilon}} $大，能够放大梯度；

（2） 后期$ g_t $较大的时候，较$ -\frac{1}{\sqrt{\sum_{i=1}^t(g_i)^2 + \epsilon}} $小，能够约束梯度；

（3） 适合处理稀疏梯度。

***

**缺点**：

（1） 由公式可以看出，仍依赖于人工设置一个全局学习率$ \eta $；

（2） $ \eta $设置过大，会使$ -\frac{1}{\sqrt{\sum_{i=1}^t(g_i)^2 + \epsilon}} $过于敏感，对梯度的调节太大；

（3） 中后期，分母上梯度平方的累加将会越来越大，使$ \text{gradient} \rightarrow 0 $，使得训练提前结束。

***

#### 6. Adadelta

Adadelta是对Adagrad的扩展，仍是对学习率进行自适应约束。但Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项。

$$ \upsilon_t = \beta \upsilon_{t-1} + (1-\beta)g_{t}^2 $$

$$ \Delta \theta_t = -\frac{\eta}{\sqrt{\upsilon_t + \epsilon}}g_t $$



#### References

* [从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)
* [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270)
* [深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)
* [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
* [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/) 
* [Pytorch中常用的四种优化器SGD、Momentum、RMSProp、Adam](https://cloud.tencent.com/developer/article/1491393)
* [深度学习（九） 深度学习最全优化方法总结比较（SGD，Momentum，Nesterov Momentum，Adagrad，Adadelta，RMSprop，Adam）](https://www.bbsmax.com/A/A7zgplBkJ4/)

