---
title: 深度学习优化算法
author: 赵旭山
tags: PyTorch
---

本文主要参考：

[https://zhuanlan.zhihu.com/p/32626442](https://zhuanlan.zhihu.com/p/32626442)

[https://zhuanlan.zhihu.com/p/22252270](https://zhuanlan.zhihu.com/p/22252270)

#### 1. 概览

对机器学习的算法一知半解，本文是个人总结整理备忘。引用两张[文献](https://arxiv.org/pdf/1609.04747.pdf)中的动态图：

![](/assets/images/optimizationAlgorithmOfDNN.gif)

![](/assets/images/lossSurfaceOfDNN.gif)

> "最优化问题是计算数学中最为重要的研究方向之一。而在深度学习领域，优化算法的选择也是一个模型的重中之重。即使在数据集和模型架构完全相同的情况下，采用不同的优化算法，也很可能导致截然不同的训练效果。
>
> 梯度下降是目前神经网络中使用最为广泛的优化算法之一。为了弥补朴素梯度下降的种种缺陷，研究者们发明了一系列变种算法，从最初的 SGD (随机梯度下降) 逐步演进到 NAdam。然而，许多学术界最为前沿的文章中，都并没有一味使用 Adam/NAdam  等公认“好用”的自适应算法，很多甚至还选择了最为初级的 SGD 或者 SGD with Momentum 等。"

#### 2. 梯度下降（Gradient Descent，GD）

梯度下降算法通过沿梯度的相反方向更新模型参数，学习率$ \eta $为每一时刻的更新步长。[此文](https://zhuanlan.zhihu.com/p/32626442)给出了梯度下降的流程：

（1） 计算目标函数关于参数的梯度

$$ g_t = \nabla_\theta J(\theta) $$

（2） 根据**历史**梯度计算一阶和二阶动量

$$ m_t = \phi(g_1, g_2, ..., g_t) $$

$$ \upsilon_t = \psi(g_1, g_2, ..., g_t) $$

（3） 更新模型参数

$$ \theta_{t+1} = \theta_t - \frac{1}{\sqrt{\upsilon_t + \epsilon}}m_t $$

其中，$ \epsilon $为平滑项，防止分母为零，通常取$ 10^{-8} $。

#### 2. Gradient Descent变种算法

梯度下降最常见的三种变形：Batch Gradient Descent（BGD）、Stochastic Gradient Descent（SGD）、Mini-Batch Gradient Descent（MBGD），这三种形式的区别就是用多少数据来计算目标函数的梯度。

最常用的为：**随机梯度下降（Stochastic Gradient Descent，SGD）**，SGD的数学形式如下：

$$ m_t = \eta g_t $$

$$ \upsilon_t = I^2 $$

$$ \epsilon = 0 $$

更新步骤为：

$$ \theta_{i+1} = \theta_t - \eta g_t $$

SGD的缺点在于收敛速度慢，可能会在鞍点处震荡。并且，如何合理地选择学习率是SGD的一大难点。

#### 3. Momentum

SGD容易陷入局部最优的沟壑中震荡，故引入动量Momentum加速SGD在正确的方向上下降并抑制震荡。

$$ m_t = \gamma m_{t-1} + \eta g_t $$

Momentum（SGD + momentum）即在原步长的基础上，加上了与上一时刻步长相关的$ \gamma m_{t-1} $，$ \gamma $取0.9左右。

> 这意味着参数更新方向不仅由当前的梯度决定，也与此前累积的下降方向有关。这使得参数中那些梯度方向变化不大的维度可以加速更新，并减少梯度方向变化较大的维度上的更新幅度。由此产生了加速收敛和减小震荡的效果。

#### 4. Nesterov Accelerated Gradient (NAG)

NAG算法在目标函数有增高趋势之前，减缓更新速率，从而使下降的过程更加智能。

$$ g_t = \nabla_{\theta} J (\theta - \gamma m_{t-1}) $$

![](/assets/images/NAGUpdate202003061737.jpg)

NAG算法流程描述如下：

* 首先基于Momentum计算一个梯度（短的蓝色向量），然后在加速更新梯度的方向进行一个大的跳跃（长的蓝色向量）；
* 计算出下一时刻$ \theta $的近似位置（棕向量），并根据该未来位置计算梯度（红向量），然后使用和Momentum中相同的方式计算步长（绿向量）；
* 这种计算梯度的方式可以使算法更好地**预测未来**，提前调整更新速率。

NAG在Momentum的基础上进一步改进了梯度计算公式：

$$ g_t = \nabla_{\theta}J(\theta - \gamma m_{t-1}) $$

$$ m_t = \gamma m_{t-1} + g_t $$

$$ \theta_t = \theta_{t-1} - \eta m_t $$



#### References

* [从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)
* [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270)
* [深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)
* [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
* [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/) 
* [Pytorch中常用的四种优化器SGD、Momentum、RMSProp、Adam](https://cloud.tencent.com/developer/article/1491393)
* [深度学习（九） 深度学习最全优化方法总结比较（SGD，Momentum，Nesterov Momentum，Adagrad，Adadelta，RMSprop，Adam）](https://www.bbsmax.com/A/A7zgplBkJ4/)

