---
title: Pytorch基本操作
author: 赵旭山
tags: Pytorch
---

#### 1. Pytorch基本操作概要

搜索网页学习Pytorch中看到一张结构清晰的思维导图，作者总结得很清晰，引用过来学习学习。如下：

Figure source: [https://zhuanlan.zhihu.com/p/36233589](https://zhuanlan.zhihu.com/p/36233589)

![](/assets/images/pytorchflowchart202002272158.jpg)

#### 2. Tensor属性

```python
x = torch.rand(5, 3)
x.dtype
x.device
x.layout
x.stride()
```

> torch.float32
>
> device(type='cpu')
>
> torch.strided
>
> (3, 1)

dtype就是数据的类型，Pytorch共有八种数据类型：

| Data type                | dtype                         | dtype                |
| ------------------------ | ----------------------------- | -------------------- |
| 32-bit floating point    | torch.float32 or torch.float  | torch.*.FloatTensor  |
| 64-bit floating point    | torch.float64 or torch.double | torch.*.DoubleTensor |
| 16-bit floating point    | torch.float16 or torch.half   | torch.*.HalfTensor   |
| 8-bit integer (unsigned) | torch.uint8                   | torch.*.ByteTensor   |
| 8-bit integer (signed)   | torch.int8                    | torch.*.CharTensor   |
| 16-bit integer (signed)  | torch.int16 or torch.short    | torch.*.ShortTensor  |
| 32-bit integer (signed)  | torch.int32 or torch.int      | torch.*.IntTensor    |
| 64-bit integer (signed)  | torch.int64 or torch.long     | torch.*.LongTensor   |

layout是数据在内存中是怎么存的，分为稀疏张量和致密张量，常用的为致密张量（`torch.strided`）。

笔者**朴素的理解**就是定义的变量在内存中咋存储的，对二维矩阵而言，用了多少列，行则被展平存成一行。`x.t()`等同于矩阵转置，没存，只把两个数据交换了一下输出。

```python
In [7]: x.stride()                                              
Out[7]: (3, 1)

In [8]: x                                                       
Out[8]: 
tensor([[0.6031, 0.0937, 0.3282],
        [0.2758, 0.8520, 0.1412],
        [0.9676, 0.9618, 0.5699],
        [0.1657, 0.2150, 0.9051],
        [0.7828, 0.9131, 0.4832]])

In [9]: x.stride()                                              
Out[9]: (3, 1)

In [10]: x.t()                                                  
Out[10]: 
tensor([[0.6031, 0.2758, 0.9676, 0.1657, 0.7828],
        [0.0937, 0.8520, 0.9618, 0.2150, 0.9131],
        [0.3282, 0.1412, 0.5699, 0.9051, 0.4832]])

In [11]: x.t().stride()                                         
Out[11]: (1, 3)
```

对于三维张量，后两维相乘是第一个数，最后一维是第二个数，最后一个数为1。t()函数只适用于2维及以下的张量，三维及以上不适用。

```python
In [30]: x =torch.rand(3, 8, 5)                                 

In [31]: x.size()                                               
Out[31]: torch.Size([3, 8, 5])

In [32]: x.stride()                                             
Out[32]: (40, 5, 1)

In [33]: xx = torch.rand(6, 3, 7)                               

In [34]: xx.size()                                              
Out[34]: torch.Size([6, 3, 7])

In [35]: xx.stride()                                            
Out[35]: (21, 7, 1)
  
In [36]: xx.t()                                                 
----------------------------------------------------------------
RuntimeError                   Traceback (most recent call last)
<ipython-input-36-0b88b5bf88e4> in <module>
----> 1 xx.t()

RuntimeError: t() expects a tensor with <= 2 dimensions, but self is 3D

```

#### 3. 创建Tensor

##### 3.1 直接创建

**torch.tensor()**

```python
In [2]: cell = torch.tensor([[6.50, 0, 0], [0, 6.50, 0], [0, 0, 
   ...: 6.50]])                                                 

In [3]: cell                                                    
Out[3]: 
tensor([[6.5000, 0.0000, 0.0000],
        [0.0000, 6.5000, 0.0000],
        [0.0000, 0.0000, 6.5000]])
```

##### 3.2 创建类型相似但size不同的tensor

**.new()**：`inputs.new`&`torch.Tensor.new`

参考: [https://www.jb51.net/article/180679.htm](https://www.jb51.net/article/180679.htm)

创建一个新的Tensor，该Tensor的**type**和**device**都和原有Tensor一致，且无内容。

```python
In [2]: inputs = torch.rand(5, 3)                               

In [3]: inputs                                                  
Out[3]: 
tensor([[0.7991, 0.8743, 0.1387],
        [0.9594, 0.9258, 0.3077],
        [0.3318, 0.3850, 0.5850],
        [0.9942, 0.3455, 0.2891],
        [0.4658, 0.9288, 0.7334]])

In [4]: new_inputs1 = inputs.new()                               

In [5]: new_inputs1                                              
Out[5]: tensor([])

In [6]: new_inputs2 = torch.Tensor.new(inputs)                  

In [7]: new_inputs2                                             
Out[7]: tensor([])

In [8]: new_inputs3 = inputs.new(inputs.size())                 

In [9]: new_inputs3                                            
Out[9]: 
tensor([[0.0000e+00, 1.4013e-45, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00],
        [1.1704e-41, 0.0000e+00, 2.2369e+08],
        [0.0000e+00, 0.0000e+00, 0.0000e+00]])

In [10]: inputs.dtype, new_inputs1.dtype, new_inputs2.dtype, new_inputs3.dtype                                          
Out[10]: (torch.float32, torch.float32, torch.float32, torch.float32)

In [11]: inputs.device, new_inputs.device, new_inputs2.device, new_inputs3.device                                      
Out[11]: 
(device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'))
```

实际应用：**添加噪声**

可以对Tensor**添加噪声**，添加如下代码即可实现：

```python
In [17]: noise = inputs.new(inputs.size()).normal_(0, 0.01)    

In [18]: noise                                                 
Out[18]: 
tensor([[-0.0021, -0.0025,  0.0182],
        [-0.0016,  0.0106, -0.0039],
        [ 0.0023,  0.0112,  0.0024],
        [ 0.0109,  0.0007,  0.0089],
        [ 0.0177,  0.0003, -0.0026]])
```

类似的有：`.new_ones`、`.new_zeros`。

##### 3.3 从numpy生成

**torch.from_numpy()**

参考：https://www.jb51.net/article/144520.htm

```python
In [3]: np_data = np.arange(6).reshape((2, 3))                  

In [4]: np_data                                                 
Out[4]: 
array([[0, 1, 2],
       [3, 4, 5]])

# numpy 转为 pytorch格式

In [5]: torch_data = torch.from_numpy(np_data)                  

In [6]: torch_data                                              
Out[6]: 
tensor([[0, 1, 2],
        [3, 4, 5]])

# torch 转为numpy

In [7]: tensor2array = torch_data.numpy()                       

In [8]: tensor2array                                            
Out[8]: 
array([[0, 1, 2],
       [3, 4, 5]])
```

矩阵转换为Tensor：

```python
# 转为32位浮点数，torch接受的都是Tensor的形式
In [1]: data = [[1, 2], [3, 4]]                                 

In [2]: data                                                    
Out[2]: [[1, 2], [3, 4]]

In [3]: tensor = torch.FloatTensor(data)                        

In [4]: tensor                                                  
Out[4]: 
tensor([[1., 2.],
        [3., 4.]])
```

##### 3.4 创建特定Tensor

###### 3.4.1 根据矩阵要求

(1) **torch.eye()**

参考：https://www.cnblogs.com/btschang/p/10300727.html

对角线位置全1，其它位置全为0的二维矩阵。

```
In [7]: torch.eye(6)                                            
Out[7]: 
tensor([[1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1.]])
```

(2) **torch.empty()**

参考：https://www.cnblogs.com/gato-chat/p/9064974.html

empty方法创建的矩阵不是空矩阵，而是未初始化的矩阵，所以里面的值不一定是0。

```python
In [2]: x = torch.empty(5, 3)                                   

In [3]: x                                                       
Out[3]: 
tensor([[0.0000e+00, -0.0000e+00, 0.0000e+00],
        [-0.0000e+00, 1.8361e+25, 1.4603e-19],
        [1.6795e+08, 4.7423e+30, 4.7393e+30],
        [9.5461e-01, 4.4377e+27, 1.7975e+19],
        [4.6894e+27, 7.9463e+08, 3.2604e-12]])
```

(3) **torch.empty_like()**

`torch.empty_like(input)`返回和input的tensor一样size的empty张量。

```python
In [2]: x = torch.rand(5, 3)                                    

In [3]: x                                                       
Out[3]: 
tensor([[0.9643, 0.6685, 0.7706],
        [0.4562, 0.1438, 0.7719],
        [0.3813, 0.3628, 0.9026],
        [0.3653, 0.0948, 0.2917],
        [0.6953, 0.5089, 0.1560]])

In [4]: xx = torch.empty_like(x)                                

In [5]: xx                                                      
Out[5]: 
tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 1.8028e+28, 3.4740e-12],
        [1.4583e-19, 7.3470e+28, 6.1425e+28],
        [7.1441e+31, 6.9987e+22, 7.8675e+34],
        [4.7418e+30, 5.9663e-02, 7.0374e+22]])
```

###### 3.4.2 根据数值要求

(1) **torch.zeros()**

参考：https://www.cnblogs.com/gato-chat/p/9064974.html

生成0矩阵，detype参数指定了生成的数据的类型。

```python
In [9]: x = torch.zeros(5, 3, dtype=torch.long)                 

In [10]: x                                                      
Out[10]: 
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
```

(2) **torch.zeros_like()**

`torch.zeros_like(input)`返回跟input的tensor一个size的全零tensor。

```python
In [6]: xx = torch.zeros_like(x)                                

In [7]: xx                                                      
Out[7]: 
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
```

以下的`*_like`方法与以上相同，不再一一赘述。

(3) **torch.ones()**

返回全1矩阵。

```python
In [2]: x = torch.ones(5, 3)                                    

In [3]: x                                                       
Out[3]: 
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
```

(4) **torch.full()**

`torch.full(size, fill_value)`生成一个用fill_value填充的特定size张量。

```python
In [6]: x = torch.full((5, 3), 2.71828)                         

In [7]: x                                                       
Out[7]: 
tensor([[2.7183, 2.7183, 2.7183],
        [2.7183, 2.7183, 2.7183],
        [2.7183, 2.7183, 2.7183],
        [2.7183, 2.7183, 2.7183],
        [2.7183, 2.7183, 2.7183]])
```

(5) **torch.arange()** & **torch.range()**

参考：https://blog.csdn.net/m0_37586991/article/details/88830026

```python
In [2]: x = torch.range(1, 6)                                   
/usr/local/Caskroom/miniconda/base/bin/ipython:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].
  #!/usr/local/Caskroom/miniconda/base/bin/python

In [3]: x                                                       
Out[3]: tensor([1., 2., 3., 4., 5., 6.])

In [4]: y = torch.arange(1, 6)                                  

In [5]: y                                                       
Out[5]: tensor([1, 2, 3, 4, 5])

In [6]: x.dtype, y.dtype                                        
Out[6]: (torch.float32, torch.int64)
```

*  `torch.range(start=1, end=6)`的结果是会包含`end`的，而`torch.arange(start=1, end=6)`的结果并不包括`end`；

* 两者创建的`tensor`的类型也不一样。

(6) **torch.linspace()**

参考：https://blog.csdn.net/york1996/article/details/81671128

`torch.linspace(start, end, steps)`返回一个一维的tensor，这个张量包含了从start到end，分成steps个段得到的向量。输出张量的长度由steps决定。

```python
In [2]: torch.linspace(3, 10, 5)                                
Out[2]: tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])

In [3]: torch.linspace(-10, 10, steps=6)                        
Out[3]: tensor([-10.,  -6.,  -2.,   2.,   6.,  10.])
```

(7) **torch.logspace()**

`torch.logspace(start, end, steps)`返回一个一维的tensor，这个张量包含了从<img src="http://latex.codecogs.com/gif.latex?\10^{start}" />  到<img src="http://latex.codecogs.com/gif.latex?\10^{end}" />，分成steps个段得到的向量。输出张量的长度由steps决定。

```python
In [4]: torch.logspace(1, 5, 4)                                 
Out[4]: tensor([1.0000e+01, 2.1544e+02, 4.6416e+03, 1.0000e+05])

In [5]: torch.logspace(1, 5, 5)                                 
Out[5]: tensor([1.0000e+01, 1.0000e+02, 1.0000e+03, 1.0000e+04, 1.0000e+05])
```

##### 3.5 随机采样生成

(1) **torch.normal()**

参考：[https://blog.csdn.net/sxs11/article/details/81775715](https://blog.csdn.net/sxs11/article/details/81775715)

`torch.normal(mean, std)`返回一个张量，包含从给定参数means、std的离散正态分布中抽取随机数。

> mean — 均值
>
> std — 标准差

```python
In [11]: torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))                                                    
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-11-4540d51055de> in <module>
----> 1 torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))

TypeError: normal() received an invalid combination of arguments - got (std=Tensor, means=Tensor, ), but expected one of:
 * (Tensor mean, Tensor std, torch.Generator generator, Tensor out)
 * (Tensor mean, float std, torch.Generator generator, Tensor out)
 * (float mean, Tensor std, torch.Generator generator, Tensor out)
 * (float mean, float std, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)


In [12]: torch.normal(mean=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))                                                     
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-12-a4a4388458ef> in <module>
----> 1 torch.normal(mean=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))

RuntimeError: "norma_cpu" not implemented for 'Long'

In [13]: torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))                                                   
Out[13]: 
tensor([ 1.8979,  1.5922,  3.0055,  3.7566,  4.8068,  6.4627,  7.1539,  7.4512,
         9.1401, 10.0355])
```

简单分析`In [11]`、`In [12]`两种错误：

**In [11]**: [参考网页](https://blog.csdn.net/sxs11/article/details/81775715)中输入参数`means=...`不工作，改成`mean=...`就可以了，另外此处给出了`normal`函数的几种输入组合：

>  * (Tensor mean, Tensor std, torch.Generator generator, Tensor out)
>  * (Tensor mean, float std, torch.Generator generator, Tensor out)
>  * (float mean, Tensor std, torch.Generator generator, Tensor out)
>  * (float mean, float std, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)

**In [12]**: 根据上述输入组合，`std`和`mean`只接受`float`和`Tensor`两种dtype，所以`In [12]`中`mean=torch.arange(1, 11)`不工作，改成`mean=torch.arange(1., 11.)`即可。

其中：

```python
In [13]: torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))                                                   
Out[13]: 
tensor([ 1.8979,  1.5922,  3.0055,  3.7566,  4.8068,  6.4627,  7.1539,  7.4512,
         9.1401, 10.0355])
```

根据`mean`和`std`生成的张量含义如下：

```python
 1.8979 #是从均值为1，标准差为1的正态分布中随机生成的
 1.5922 #是从均值为2，标准差为0.9的正态分布中随机生成的
 3.0055 #是从均值为3，标准差为0.8的正态分布中随机生成的
 3.7566 #是从均值为4，标准差为0.7的正态分布中随机生成的
 4.8068 #是从均值为5，标准差为0.6的正态分布中随机生成的
 6.4627 #是从均值为6，标准差为0.5的正态分布中随机生成的
 7.1539 #是从均值为7，标准差为0.4的正态分布中随机生成的
 7.4512 #是从均值为8，标准差为0.3的正态分布中随机生成的
 9.1401 #是从均值为9，标准差为0.2的正态分布中随机生成的
10.0355 #是从均值为10，标准差为0.1的正态分布中随机生成的
```

<u>6.4627（均值为6，标准差为0.5）和7.4512（是从均值为8，标准差为0.3）</u>两个值有所偏差。

(2) **torch.rand()** & **torch.randn()**

torch.randn()和torch.rand()有什么区别

参考：[https://blog.csdn.net/wangwangstone/article/details/89815661](https://blog.csdn.net/wangwangstone/article/details/89815661)

torch.rand和torch.randn有什么区别？ y = torch.rand(5,3) y=torch.randn(5,3)

一个均匀分布，一个是标准正态分布。

***均匀分布***

`torch.rand(*size)`返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数。张量的形状由参数size定义。

```python
In [21]: torch.rand(size=(3, 2))                                 
Out[21]: 
tensor([[0.2218, 0.1115],
        [0.7232, 0.4805],
        [0.7677, 0.1653]])

In [22]: torch.rand(5, 3)                                       
Out[22]: 
tensor([[0.4548, 0.8788, 0.6698],
        [0.6992, 0.2677, 0.1843],
        [0.8135, 0.6389, 0.9419],
        [0.8074, 0.5690, 0.4269],
        [0.7801, 0.3185, 0.3531]])
```

***标准正态分布***

`torch.randn(*size)`返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）。

```python
In [23]: torch.randn(size=(4, 2))                               
Out[23]: 
tensor([[-2.5096,  0.3663],
        [ 0.8938,  0.3082],
        [ 0.0469,  0.3610],
        [-0.6472, -0.0929]])

In [24]: torch.randn(4, 2)                                      
Out[24]: 
tensor([[ 0.1326,  1.4875],
        [-0.3001,  0.0889],
        [ 0.5647,  0.5865],
        [ 0.6648,  0.7136]])
```

>`其他`：
>
>***离散正态分布***
>
>`torch.normal(mean, std)`，用法见前述。
>
>***线性间距向量***
>
>`torch.linspace(start, end, steps=100)`，用法见前述。

(3) **torch.randint()**

`torch.randint(low, high, size)`返回`low`～`high`之间整数特定`size`的张量。

```python
In [18]: idx = torch.randint(low=0, high=20, size=(10, 10))     

In [19]: idx                                                    
Out[19]: 
tensor([[ 6, 17, 14, 14, 14,  9,  6,  2,  2,  5],
        [19, 18, 13,  8, 14,  8,  1,  8, 17, 14],
        [ 9,  3,  1, 19,  2,  6,  1, 16,  8,  6],
        [18, 12,  0, 12,  6,  3, 14,  6, 17, 12],
        [10,  3,  9, 16,  7,  4,  5,  2, 19, 14],
        [ 3,  0, 13, 11,  0, 16,  3, 12,  7, 12],
        [ 1, 15, 15,  3, 15,  5, 13,  8,  7,  3],
        [ 8,  2, 11, 11, 13, 14,  5, 13,  0, 17],
        [ 0, 10, 15,  4, 12, 17, 19,  7, 13,  1],
        [16, 15, 12,  7, 15, 11,  8, 16, 11,  4]])

In [20]: idx.dtype                                              
Out[20]: torch.int64
```

`low`和`high`必须也是整数，否则会报错。

```python
In [21]: idx = torch.randint(low=0.5, high=20.1, size=(10, 10)) 
----------------------------------------------------------------
TypeError                      Traceback (most recent call last)
<ipython-input-21-94e2fb84d2eb> in <module>
----> 1 idx = torch.randint(low=0.5, high=20.1, size=(10, 10))

TypeError: randint() received an invalid combination of arguments - got (size=tuple, high=float, low=float, ), but expected one of:
 * (int high, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
 * (int low, int high, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
```

(4) **torch.randperm()**

参考：https://www.cnblogs.com/ymjyqsx/p/6200692.html

`torch.randperm(n)`功能是把1到n这些数随机打乱得到一个数字序列。

```python
In [22]: xrp = torch.randperm(10)                               

In [23]: xrp                                                    
Out[23]: tensor([4, 9, 2, 7, 1, 5, 6, 0, 3, 8])

In [24]: xrp.dtype                                              
Out[24]: torch.int64
```

#### 4. 操作Tensor

##### 4.1 基本操作

###### 4.1.1 indexing

(1) **torch.index_select()**

参考：[https://www.jb51.net/article/174524.htm](https://www.jb51.net/article/174524.htm)

`index_select(input, dim, index)`表示**从张量的某个维度的指定位置选取数据**。

`input`为输入的tensor，即索引的对象；`dim`表示从第几维挑选数据，0表示按行索引，1表示按列索引；`index`表示从第`dim`个参数维度中的哪个位置挑选数据。

```python
In [2]: x = torch.randint(0, 99, size=(10, 10))                 

In [3]: x                                                       
Out[3]: 
tensor([[45, 81, 16, 56, 95, 91, 37, 34, 81, 98],
        [94, 87,  7, 71, 53, 43, 86, 15, 62, 71],
        [57, 30, 78, 50, 68, 19, 12, 52,  0, 72],
        [84, 27, 44, 26, 79, 72, 32, 15, 73, 98],
        [14, 21, 40, 65, 76, 88, 89,  6, 71, 18],
        [67,  9, 27,  3, 50, 72, 56, 68, 18, 87],
        [37,  7,  5, 60,  6, 12, 17, 24, 83, 23],
        [32, 55, 79, 83, 55, 26, 44, 62, 22, 25],
        [ 3, 16, 47, 45, 18, 47, 46, 19, 16,  4],
        [50, 92, 86, 11, 90, 53,  1, 92, 22, 92]])

In [4]: torch.index_select(x, 0, 2)                             
----------------------------------------------------------------
TypeError                      Traceback (most recent call last)
<ipython-input-4-a9bfb26e8f43> in <module>
----> 1 torch.index_select(x, 0, 2)

TypeError: index_select() received an invalid combination of arguments - got (Tensor, int, int), but expected one of:
 * (Tensor input, name dim, Tensor index, Tensor out)
 * (Tensor input, int dim, Tensor index, Tensor out)

In [5]: torch.index_select(x, 0, torch.tensor([0, 2]))          
Out[5]: 
tensor([[45, 81, 16, 56, 95, 91, 37, 34, 81, 98],
        [57, 30, 78, 50, 68, 19, 12, 52,  0, 72]])

In [6]: torch.index_select(x, 0, torch.tensor([8]))                                                                               
Out[6]: tensor([[ 3, 16, 47, 45, 18, 47, 46, 19, 16,  4]])
  
In [7]: torch.index_select(x, 1, torch.tensor([0, 6]))                                                                           
Out[7]: 
tensor([[45, 37],
        [94, 86],
        [57, 12],
        [84, 32],
        [14, 89],
        [67, 56],
        [37, 17],
        [32, 44],
        [ 3, 46],
        [50,  1]])
```

从`In [4]`报错信息可知，index也须是一个Tensor：

>  * (Tensor input, name dim, Tensor index, Tensor out)
>  * (Tensor input, int dim, Tensor index, Tensor out)

dim = 0时，torch.tensor([0, 2])表示第0行和第2行，torch.tensor([8])表示第8行；

dim = 1时，torch.tensor([0, 6])表示第0行和第6行。

(2) **torch.masked_select()**

参考：[https://blog.csdn.net/q511951451/article/details/81611903](https://blog.csdn.net/q511951451/article/details/81611903)；[https://blog.csdn.net/SoftPoeter/article/details/81667810](https://blog.csdn.net/SoftPoeter/article/details/81667810)

`torch.masked_select(input, mask)`将根据张量mask中的二元值，取输入张量🀄️的指定项（mask为一个Byte Tensor），将取值返回到一个新的1D张量，**张量mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同**。返回的张量不与原始张量共享内存空间。

`input`(Tensor)为输入张量，`mask`(Byte Tensor)为掩码张量，包含了二元索引值。

```python
In [2]: x = torch.randn(5, 3)                                   

In [3]: x                                                       
Out[3]: 
tensor([[ 0.2893, -0.1957, -0.9152],
        [ 0.6943,  0.5319,  0.8884],
        [-0.8882,  0.5574,  0.7390],
        [ 1.4411,  0.3321, -0.9831],
        [ 0.9207,  0.8473, -0.4209]])

In [4]: mask = x.ge(0.5)                                        

In [5]: mask                                                    
Out[5]: 
tensor([[False, False, False],
        [ True,  True,  True],
        [False,  True,  True],
        [ True, False, False],
        [ True,  True, False]])

In [6]: torch.masked_select(x, mask)                            
Out[6]: tensor([0.6943, 0.5319, 0.8884, 0.5574, 0.7390, 1.4411, 0.9207, 0.8473])
```

###### 4.1.2 joining

(1) **torch.cat()**

参考：https://www.cnblogs.com/JeasonIsCoding/p/10162356.html

`torch.cat((tensor_A, tensor_B), dim)`表示把张量A、B拼接在一起。

***基本用法***：拼接两个Tensor

dim为0时表示按行拼接：

```python
In [2]: A = torch.ones(4, 3)                                    

In [3]: A                                                       
Out[3]: 
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])

In [4]: B = torch.zeros(2, 3)                                   

In [5]: B                                                       
Out[5]: 
tensor([[0., 0., 0.],
        [0., 0., 0.]])

In [6]: C = torch.cat((A, B), 0) # A，B须列数相同                               

In [7]: C                                                       
Out[7]: 
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [0., 0., 0.],
        [0., 0., 0.]])
```

dim为1时表示按列拼接：

```python
In [2]: A = torch.ones(4, 3)                                    

In [3]: A                                                       
Out[3]: 
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])

In [8]: AA = torch.ones(4, 2) * 2                               

In [9]: AA                                                      
Out[9]: 
tensor([[2., 2.],
        [2., 2.],
        [2., 2.],
        [2., 2.]])

In [10]: CC = torch.cat((A, AA), 1) #A，AA须行数相同                            

In [11]: CC                                                     
Out[11]: 
tensor([[1., 1., 1., 2., 2.],
        [1., 1., 1., 2., 2.],
        [1., 1., 1., 2., 2.],
        [1., 1., 1., 2., 2.]])
```

***其他用法***：拼接两个list

```python
In [2]: x = torch.Tensor([[1], [2], [3]])                       

In [3]: x                                                       
Out[3]: 
tensor([[1.],
        [2.],
        [3.]])

In [4]: x1 = [x*2 for i in range(1, 4)]                         

In [5]: x1                                                      
Out[5]: 
[tensor([[2.],
         [4.],
         [6.]]),
 tensor([[2.],
         [4.],
         [6.]]),
 tensor([[2.],
         [4.],
         [6.]])]

In [6]: xx1 = torch.cat((x1), 1)                                

In [7]: xx1                                                     
Out[7]: 
tensor([[2., 2., 2.],
        [4., 4., 4.],
        [6., 6., 6.]])

In [8]: xx1_1 = torch.cat(x1, 0)                               

In [9]: xx1_1                                                  
Out[9]: 
tensor([[2.],
        [4.],
        [6.],
        [2.],
        [4.],
        [6.],
        [2.],
        [4.],
        [6.]])
```

(2) **torch.stack()**

参考：[https://www.cnblogs.com/yifdu25/p/9399047.html](https://www.cnblogs.com/yifdu25/p/9399047.html)；[https://blog.csdn.net/Teeyohuang/article/details/80362756](https://blog.csdn.net/Teeyohuang/article/details/80362756)

`torch.stack()`与`torch.cat()`的区别在于：

- `cat`对数据沿某一维度进行拼接，`cat`后数据的总维度数不变；
- `stack`为增加新的维度进行堆叠。

如对两个1*2维的tensor在第0个维度上`stack`，则会变成2\*2\*2的tensor；在第1个维度上`stack`，则会变成1\*2\*2的tensor。

```python
In [2]: a = torch.rand(1, 2)                                    

In [3]: a                                                       
Out[3]: tensor([[0.5289, 0.9828]])

In [4]: b = torch.rand((1, 2))                                  

In [5]: b                                                       
Out[5]: tensor([[0.1610, 0.5676]])

In [6]: c = torch.stack((a, b), 0)                              

In [7]: c                                                       
Out[7]: 
tensor([[[0.5289, 0.9828]],

        [[0.1610, 0.5676]]])
```

对于高维度张量：

```python
In [2]: a = torch.randn((1, 2, 3, 4))                           

In [3]: a                                                       
Out[3]: 
tensor([[[[ 0.3603, -0.4322, -0.3183, -0.9004],
          [-0.5849,  1.2128, -1.0935,  0.4143],
          [ 1.4446,  1.3867,  1.8066, -2.2650]],

         [[ 2.2352, -1.2699, -0.5572, -1.8167],
          [-0.5426, -0.1514,  0.1314, -1.2296],
          [ 0.0166,  0.6789, -0.6616, -0.6927]]]])

In [4]: b = torch.stack((a, a), 0)                              

In [5]: b                                                       
Out[5]: 
tensor([[[[[ 0.3603, -0.4322, -0.3183, -0.9004],
           [-0.5849,  1.2128, -1.0935,  0.4143],
           [ 1.4446,  1.3867,  1.8066, -2.2650]],

          [[ 2.2352, -1.2699, -0.5572, -1.8167],
           [-0.5426, -0.1514,  0.1314, -1.2296],
           [ 0.0166,  0.6789, -0.6616, -0.6927]]]],



        [[[[ 0.3603, -0.4322, -0.3183, -0.9004],
           [-0.5849,  1.2128, -1.0935,  0.4143],
           [ 1.4446,  1.3867,  1.8066, -2.2650]],

          [[ 2.2352, -1.2699, -0.5572, -1.8167],
           [-0.5426, -0.1514,  0.1314, -1.2296],
           [ 0.0166,  0.6789, -0.6616, -0.6927]]]]])

In [6]: a.shape, b.shape                                        
Out[6]: (torch.Size([1, 2, 3, 4]), torch.Size([2, 1, 2, 3, 4]))

In [7]: c = torch.stack((a, a), 1)                              

In [8]: c                                                       
Out[8]: 
tensor([[[[[ 0.3603, -0.4322, -0.3183, -0.9004],
           [-0.5849,  1.2128, -1.0935,  0.4143],
           [ 1.4446,  1.3867,  1.8066, -2.2650]],

          [[ 2.2352, -1.2699, -0.5572, -1.8167],
           [-0.5426, -0.1514,  0.1314, -1.2296],
           [ 0.0166,  0.6789, -0.6616, -0.6927]]],


         [[[ 0.3603, -0.4322, -0.3183, -0.9004],
           [-0.5849,  1.2128, -1.0935,  0.4143],
           [ 1.4446,  1.3867,  1.8066, -2.2650]],

          [[ 2.2352, -1.2699, -0.5572, -1.8167],
           [-0.5426, -0.1514,  0.1314, -1.2296],
           [ 0.0166,  0.6789, -0.6616, -0.6927]]]]])

In [9]: a.shape, c.shape                                        
Out[9]: (torch.Size([1, 2, 3, 4]), torch.Size([1, 2, 2, 3, 4]))

In [10]: d = torch.stack((a, a), 3)                             

In [11]: a.shape, d.shape                                       
Out[11]: (torch.Size([1, 2, 3, 4]), torch.Size([1, 2, 3, 2, 4]))
```

根据高维度张量的`stack`可知，dim=0即在第0个维度前增加一个维度，dim=1即在第1个维度之前增加一个维度，...，依此类推。

(3) **torch.gather()**

参考：[https://blog.csdn.net/edogawachia/article/details/80515038](https://blog.csdn.net/edogawachia/article/details/80515038)；https://blog.csdn.net/Lucky_Rocks/article/details/79676095。

`torch.gather(input, dim, index)`表示沿给定的dim，将输入索引张量index指定位置的值进行聚合。

```python
In [18]: t = torch.Tensor([[1,2,3],[4,5,6]])                    

In [19]: t                                                      
Out[19]: 
tensor([[1., 2., 3.],
        [4., 5., 6.]])

In [20]: torch.gather(t, 1, torch.LongTensor([[1,0],[0,0]]))    
Out[20]: 
tensor([[2., 1.],
        [4., 4.]])
```

根据以上示例代码理解此函数：

* `dim=1`按列取；
* 第一组index为[1, 0]，第1列（实际的第二列）为2，第0列为1，所以第一行为[2, 1]；
* 同理，第二组index为[0, 0]，第0列为4，第0列为4，所以第二行为[4, 4]。
* 所以，最终结果为`[[2., 1.], [4., 4.]]`。

###### 4.1.3 slicing

(1) **torch.split()**

`torch.split(tensor, split_size, dim=0)`将输入张量分割为相等形状的块。

`split_size`表示需要切分的大小，可以为**`int`**和**`list`**两种类型。

***当`split_size`为`int`整数时***：

如果沿指定维的张量形状大小不能被split_size整分，则最后一个分块会小于其它分块。

```python
In [2]: x = torch.randn(3, 10, 6)                               

In [3]: x                                                       
Out[3]: 
tensor([[[-0.8775, -0.4960,  1.2960, -0.7834,  1.1593,  0.8485],
         [-1.3548, -1.0891,  0.3011,  0.5822,  0.1337, -0.9977],
         [-0.6412, -0.3102, -0.1618,  1.1270, -0.9163,  0.7583],
         [ 0.9933,  0.9254,  1.6719, -0.2064,  0.1526, -0.5263],
         [ 0.7706, -0.9688,  0.8912,  1.9223,  1.9731, -0.6831],
         [-0.0069,  0.2784,  1.2610, -0.3232, -1.1976, -0.1344],
         [ 0.0777,  0.7018, -0.5435, -0.8737, -0.4440,  1.0564],
         [ 0.1293, -0.8249,  0.7455,  1.2296,  0.4821,  1.3589],
         [ 0.3864, -1.4730,  0.6603,  0.5465,  0.1333, -0.7331],
         [ 1.7947, -0.5429, -1.0085,  0.6933,  0.6731, -1.2255]],

        [[ 0.0998, -0.0930, -0.4057,  0.2446,  0.7161,  0.3036],
         [-1.9477,  2.1896,  0.2929,  0.5595,  1.5185,  1.1661],
         [ 0.1391,  0.2492, -0.0041,  0.4088,  0.9216,  0.7747],
         [ 0.5324,  0.8214, -1.0135,  0.2731, -0.2142, -0.0330],
         [-0.7933,  1.4916,  0.0318,  0.5016, -0.0532, -1.2558],
         [-0.0261, -0.6143,  0.0034, -2.4476,  0.5431,  0.9995],
         [-1.4268,  1.0937, -1.7195,  0.9329,  2.3844,  0.0674],
         [ 1.2903,  1.1133, -0.9451, -0.7562, -0.8685, -0.6552],
         [ 0.7247, -2.1444, -0.0583,  0.0145,  0.6785, -0.3694],
         [ 1.3159,  0.1323, -1.5535,  1.4214, -0.7329, -0.6577]],

        [[ 0.8594, -0.7541,  0.4966, -0.2951, -0.7567,  1.3233],
         [-0.1438, -1.6067,  0.0128,  0.6936,  0.3204, -1.0199],
         [-0.4498,  1.3496,  0.7266,  0.3077,  0.1027,  0.2071],
         [ 0.8560,  0.6274,  1.2768, -0.9968,  0.6768, -0.5088],
         [ 0.2686, -0.3681, -1.1322,  0.9814,  0.9188, -0.1681],
         [-0.6616, -0.7909, -0.0702, -0.4303,  2.8490,  0.7392],
         [ 0.7906,  0.3060, -0.2649, -0.2921,  0.4829, -0.1724],
         [-0.1080,  0.9259,  0.0673, -0.4310,  0.1326,  0.2177],
         [ 0.8790,  1.0464,  1.4730, -1.7451,  1.1794, -0.0900],
         [-0.3713,  0.9280,  0.5084, -0.1260, -0.5660, -2.3400]]])

In [4]: a, b, c = x.split(1, 0)                                 

In [5]: a                                                       
Out[5]: 
tensor([[[-0.8775, -0.4960,  1.2960, -0.7834,  1.1593,  0.8485],
         [-1.3548, -1.0891,  0.3011,  0.5822,  0.1337, -0.9977],
         [-0.6412, -0.3102, -0.1618,  1.1270, -0.9163,  0.7583],
         [ 0.9933,  0.9254,  1.6719, -0.2064,  0.1526, -0.5263],
         [ 0.7706, -0.9688,  0.8912,  1.9223,  1.9731, -0.6831],
         [-0.0069,  0.2784,  1.2610, -0.3232, -1.1976, -0.1344],
         [ 0.0777,  0.7018, -0.5435, -0.8737, -0.4440,  1.0564],
         [ 0.1293, -0.8249,  0.7455,  1.2296,  0.4821,  1.3589],
         [ 0.3864, -1.4730,  0.6603,  0.5465,  0.1333, -0.7331],
         [ 1.7947, -0.5429, -1.0085,  0.6933,  0.6731, -1.2255]]])

In [6]: b                                                       
Out[6]: 
tensor([[[ 0.0998, -0.0930, -0.4057,  0.2446,  0.7161,  0.3036],
         [-1.9477,  2.1896,  0.2929,  0.5595,  1.5185,  1.1661],
         [ 0.1391,  0.2492, -0.0041,  0.4088,  0.9216,  0.7747],
         [ 0.5324,  0.8214, -1.0135,  0.2731, -0.2142, -0.0330],
         [-0.7933,  1.4916,  0.0318,  0.5016, -0.0532, -1.2558],
         [-0.0261, -0.6143,  0.0034, -2.4476,  0.5431,  0.9995],
         [-1.4268,  1.0937, -1.7195,  0.9329,  2.3844,  0.0674],
         [ 1.2903,  1.1133, -0.9451, -0.7562, -0.8685, -0.6552],
         [ 0.7247, -2.1444, -0.0583,  0.0145,  0.6785, -0.3694],
         [ 1.3159,  0.1323, -1.5535,  1.4214, -0.7329, -0.6577]]])

In [7]: c                                                       
Out[7]: 
tensor([[[ 0.8594, -0.7541,  0.4966, -0.2951, -0.7567,  1.3233],
         [-0.1438, -1.6067,  0.0128,  0.6936,  0.3204, -1.0199],
         [-0.4498,  1.3496,  0.7266,  0.3077,  0.1027,  0.2071],
         [ 0.8560,  0.6274,  1.2768, -0.9968,  0.6768, -0.5088],
         [ 0.2686, -0.3681, -1.1322,  0.9814,  0.9188, -0.1681],
         [-0.6616, -0.7909, -0.0702, -0.4303,  2.8490,  0.7392],
         [ 0.7906,  0.3060, -0.2649, -0.2921,  0.4829, -0.1724],
         [-0.1080,  0.9259,  0.0673, -0.4310,  0.1326,  0.2177],
         [ 0.8790,  1.0464,  1.4730, -1.7451,  1.1794, -0.0900],
         [-0.3713,  0.9280,  0.5084, -0.1260, -0.5660, -2.3400]]])

In [8]: d, e = x.split(2, 0)                                    

In [9]: d                                                       
Out[9]: 
tensor([[[-0.8775, -0.4960,  1.2960, -0.7834,  1.1593,  0.8485],
         [-1.3548, -1.0891,  0.3011,  0.5822,  0.1337, -0.9977],
         [-0.6412, -0.3102, -0.1618,  1.1270, -0.9163,  0.7583],
         [ 0.9933,  0.9254,  1.6719, -0.2064,  0.1526, -0.5263],
         [ 0.7706, -0.9688,  0.8912,  1.9223,  1.9731, -0.6831],
         [-0.0069,  0.2784,  1.2610, -0.3232, -1.1976, -0.1344],
         [ 0.0777,  0.7018, -0.5435, -0.8737, -0.4440,  1.0564],
         [ 0.1293, -0.8249,  0.7455,  1.2296,  0.4821,  1.3589],
         [ 0.3864, -1.4730,  0.6603,  0.5465,  0.1333, -0.7331],
         [ 1.7947, -0.5429, -1.0085,  0.6933,  0.6731, -1.2255]],

        [[ 0.0998, -0.0930, -0.4057,  0.2446,  0.7161,  0.3036],
         [-1.9477,  2.1896,  0.2929,  0.5595,  1.5185,  1.1661],
         [ 0.1391,  0.2492, -0.0041,  0.4088,  0.9216,  0.7747],
         [ 0.5324,  0.8214, -1.0135,  0.2731, -0.2142, -0.0330],
         [-0.7933,  1.4916,  0.0318,  0.5016, -0.0532, -1.2558],
         [-0.0261, -0.6143,  0.0034, -2.4476,  0.5431,  0.9995],
         [-1.4268,  1.0937, -1.7195,  0.9329,  2.3844,  0.0674],
         [ 1.2903,  1.1133, -0.9451, -0.7562, -0.8685, -0.6552],
         [ 0.7247, -2.1444, -0.0583,  0.0145,  0.6785, -0.3694],
         [ 1.3159,  0.1323, -1.5535,  1.4214, -0.7329, -0.6577]]])

In [10]: e                                                      
Out[10]: 
tensor([[[ 0.8594, -0.7541,  0.4966, -0.2951, -0.7567,  1.3233],
         [-0.1438, -1.6067,  0.0128,  0.6936,  0.3204, -1.0199],
         [-0.4498,  1.3496,  0.7266,  0.3077,  0.1027,  0.2071],
         [ 0.8560,  0.6274,  1.2768, -0.9968,  0.6768, -0.5088],
         [ 0.2686, -0.3681, -1.1322,  0.9814,  0.9188, -0.1681],
         [-0.6616, -0.7909, -0.0702, -0.4303,  2.8490,  0.7392],
         [ 0.7906,  0.3060, -0.2649, -0.2921,  0.4829, -0.1724],
         [-0.1080,  0.9259,  0.0673, -0.4310,  0.1326,  0.2177],
         [ 0.8790,  1.0464,  1.4730, -1.7451,  1.1794, -0.0900],
         [-0.3713,  0.9280,  0.5084, -0.1260, -0.5660, -2.3400]]])

In [11]: a.dtype, b.dtype, c.dtype, d.dtype, e.dtype            
Out[11]: (torch.float32, torch.float32, torch.float32, torch.float32, torch.float32)
  
In [12]: a.shape, b.shape, c.shape, d.shape, e.shape            
Out[12]: 
(torch.Size([1, 10, 6]),
 torch.Size([1, 10, 6]),
 torch.Size([1, 10, 6]),
 torch.Size([2, 10, 6]),
 torch.Size([1, 10, 6]))
```

逐条看各个命令：

```python
x = torch.randn(3, 10, 6) # 生成一个(3, 10, 6)的x张量
a, b, c = x.split(1, 0)   # 基于第0维，把x分为3个张量，于是每个张量shape俱为(1, 10, 6)
d, e = x.split(2, 0)      # 基于第0维，把x分为2个张量，第一个张量的shape为(2, 10, 6)，剩下的张量split_size不足2了，所以第二个张量的shape为(1, 10, 6)
```

***当`split_size`为`list`列表时***：

参考：[https://www.codetd.com/article/8938026](https://www.codetd.com/article/8938026)

```python
In [2]: x = torch.rand(3, 8, 6)                                 

In [3]: xa, xb, xc = torch.split(x, [2, 1, 5], dim=1)           

In [4]: xa.shape, xb.shape, xc.shape                            
Out[4]: (torch.Size([3, 2, 6]), torch.Size([3, 1, 6]), torch.Size([3, 5, 6]))
```

如上实例中所示，张量x基于第1维（实际的第**二**维）被分为`[2, 1, 5]`三个张量，此例中，x的第二维为8，所以分成的三个张量加和也必须等于8，即`2+1+5=8`。否则，会报错。

```python
In [5]: xi = torch.split(x, [2, 1, 1], dim=1)                                                                                     
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-5-5cd84dcf8582> in <module>
----> 1 xi = torch.split(x, [2, 1, 1], dim=1)

/usr/local/Caskroom/miniconda/base/lib/python3.7/site-packages/torch/functional.py in split(tensor, split_size_or_sections, dim)
     85     # split_size_or_sections. The branching code is in tensor.py, which we
     86     # call here.
---> 87     return tensor.split(split_size_or_sections, dim)
     88 
     89 # equivalent to itertools.product(indices)

/usr/local/Caskroom/miniconda/base/lib/python3.7/site-packages/torch/tensor.py in split(self, split_size, dim)
    384                 return super(Tensor, self).split_with_sizes(split_size, dim)
    385         else:
--> 386             return super(Tensor, self).split_with_sizes(split_size, dim)
    387 
    388     def unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None):

RuntimeError: split_with_sizes expects split_sizes to sum exactly to 8 (input tensor's size at dimension 1), but got split_sizes=[2, 1, 1]
```

报错信息：`RuntimeError: split_with_sizes expects split_sizes to sum exactly to 8 (input tensor's size at dimension 1), but got split_sizes=[2, 1, 1]`，要求分割`list`各值的加和与张量相应维度的值必须相等。

(2) **torch.chunk()**

参考：[https://www.codetd.com/article/8938026](https://www.codetd.com/article/8938026)

`torch.chunk(input, chunks)`基本使用和`torch.split()`相同。`input`为需要切分的tensor，`chunks`(int型)为需要切分后的块大小，`dim`为切分的维度。

区别：

* `chunks`只能是`int`型，而`torch.split`中`split_size`可以是list；
* `chunks`在块值小于该维度值时，会将块按照维度切分成1的结构。而split则会返回原来的张量。

```python
In [2]: x = torch.rand(2, 4, 6)                                 

In [3]: y = torch.chunk(x, 5, dim=1)                            

In [6]: for i in y: 
   ...:     print(i.size()) 
   ...:                                                         
torch.Size([2, 1, 6])
torch.Size([2, 1, 6])
torch.Size([2, 1, 6])
torch.Size([2, 1, 6])

In [7]: x.size()                                                
Out[7]: torch.Size([2, 4, 6])

In [8]: yy = torch.split(x, 5, dim=1) 
  
In [11]: for i in yy: 
    ...:     print(i.size()) 
    ...:                                                                                                                          
torch.Size([2, 4, 6])
```

###### 4.1.4 mutation

(1) **torch.transpose()**

参考：[https://www.cnblogs.com/yifdu25/p/9399047.html](https://www.cnblogs.com/yifdu25/p/9399047.html)

交换维度。

```python
In [2]: x = torch.randn(2, 3)                                   

In [3]: x                                                       
Out[3]: 
tensor([[-0.2395, -1.3414, -0.7900],
        [ 0.0117,  0.5133, -0.1420]])

In [4]: x.transpose(0, 1)                                       
Out[4]: 
tensor([[-0.2395,  0.0117],
        [-1.3414,  0.5133],
        [-0.7900, -0.1420]])
```

(2) **torch.t()**

参考：[https://blog.csdn.net/foneone/article/details/103876131](https://blog.csdn.net/foneone/article/details/103876131)

`torch.t()`是一个类似于求矩阵的转置的函数，但是它要求输入的tensor结构维度<=2D。

当输入张量维度为2维时，输出转置后的张量，如下所示：

```python
In [2]: x = torch.rand(1, 5)                                    

In [3]: x                                                       
Out[3]: tensor([[0.0980, 0.8057, 0.1076, 0.5593, 0.3623]])

In [4]: x.t()                                                   
Out[4]: 
tensor([[0.0980],
        [0.8057],
        [0.1076],
        [0.5593],
        [0.3623]])
```

输入张量为1维时，不做操作，输出原张量：

```python
In [5]: x = torch.rand(5)                                       

In [6]: x                                                       
Out[6]: tensor([0.3640, 0.5733, 0.0280, 0.4891, 0.6522])

In [7]: x.size()                                                
Out[7]: torch.Size([5])

In [8]: x.t()                                                   
Out[8]: tensor([0.3640, 0.5733, 0.0280, 0.4891, 0.6522])

In [9]: x.t().size()                                            
Out[9]: torch.Size([5])
```

(3) **torch.squeeze()** & **torch.unsqueeze()**

参考:

[https://www.cnblogs.com/yifdu25/p/9399047.html](https://www.cnblogs.com/yifdu25/p/9399047.html)；

[https://blog.csdn.net/xiexu911/article/details/80820028](https://blog.csdn.net/xiexu911/article/details/80820028)

squeeze和unsqueeze的用法主要是对数据的维度进行压缩或者解压。（这句话一点营养都没有🤭🤭😬)

函数功能：去掉size为1的维度，包括行和列。**当维度大于等于2时，squeeze()无作用**，**当维度大于等于2时，squeeze()无作用**。

其中，`squeeze(0)`代表第一维度值为1则去除第一维度，`squeeze(1)`代表若第二维度为1则去除第二维度。

用实例学习最靠谱：

```python
In [2]: x = torch.randn(1, 3)                                                                                                     
In [3]: x                                                                                                                         
Out[3]: tensor([[-0.0217,  1.5953,  0.5264]])

In [4]: xs = x.squeeze(0)                                                                                                         
In [5]: xs                                                                                                                        
Out[5]: tensor([-0.0217,  1.5953,  0.5264])

In [6]: x.size(), xs.size()                                                                                                       
Out[6]: (torch.Size([1, 3]), torch.Size([3]))
```

上述实例去除了第一维度，不太明显，继续：

```python
In [2]: x = torch.randn(3, 1)                                   

In [3]: x                                                       
Out[3]: 
tensor([[-0.1502],
        [ 0.0505],
        [ 0.1177]])

In [4]: xx = x.squeeze(1)                                       

In [5]: xx                                                      
Out[5]: tensor([-0.1502,  0.0505,  0.1177])

In [6]: x.size(), xx.size()                                     
Out[6]: (torch.Size([3, 1]), torch.Size([3]))
```

第二维度被去掉了，列变成行了，这个用得比较多。

`unqueeze`把上述操作恢复回来：

```python
In [9]: xx.unsqueeze(1)                                         
Out[9]: 
tensor([[-0.1502],
        [ 0.0505],
        [ 0.1177]])
        
In [10]: xx.unsqueeze(0)                                        
Out[10]: tensor([[-0.1502,  0.0505,  0.1177]])
```

自己体会吧，有点表达不出来了🥵。

(4) **torch.reshape()**

参考：[https://blog.csdn.net/GhostintheCode/article/details/102531237](https://blog.csdn.net/GhostintheCode/article/details/102531237)

`torch.reshape(input, shape)`返回具有与input相同的数据和元素数量的张量，具有指定形状shape。

```python
In [2]: a = torch.arange(6.)                                    

In [3]: a                                                       
Out[3]: tensor([0., 1., 2., 3., 4., 5.])

In [4]: torch.reshape(a, (2, 3))                                
Out[4]: 
tensor([[0., 1., 2.],
        [3., 4., 5.]])
```

若shape只有一个-1，就是直接展开。

```python
In [2]: a = torch.tensor([[1, 2], [3, 4]])                      

In [3]: a                                                       
Out[3]: 
tensor([[1, 2],
        [3, 4]])

In [4]: torch.reshape(a, (-1,))  # shape只有一个-1                         
Out[4]: tensor([1, 2, 3, 4])     # 张量直接展开
```

若shape某个尺寸为-1，则剩余尺寸根据输入的元素推断出来。

```python
In [2]: a = torch.arange(8.)                                    

In [3]: a                                                       
Out[3]: tensor([0., 1., 2., 3., 4., 5., 6., 7.])

In [4]: torch.reshape(a, (-1, 4)) # 给定一个，另一个能算出来                 
Out[4]: 
tensor([[0., 1., 2., 3.],
        [4., 5., 6., 7.]])
```

(5) **torch.unbind()**

参考：[https://zhuanlan.zhihu.com/p/36233589](https://zhuanlan.zhihu.com/p/36233589)

解除dim绑定，把张量进行拆分。

```python
In [2]: x = torch.randn((5, 3))                                 

In [3]: x                                                       
Out[3]: 
tensor([[ 0.3580,  0.9733, -0.5895],
        [ 0.3258,  0.6766, -0.8588],
        [-0.1140, -1.4491, -0.4318],
        [-0.6378, -0.5162,  1.5366],
        [ 0.6458, -0.3919, -1.5191]])

In [4]: x.unbind()                                              
Out[4]: 
(tensor([ 0.3580,  0.9733, -0.5895]),
 tensor([ 0.3258,  0.6766, -0.8588]),
 tensor([-0.1140, -1.4491, -0.4318]),
 tensor([-0.6378, -0.5162,  1.5366]),
 tensor([ 0.6458, -0.3919, -1.5191]))
```

(6) **torch.where()**

参考：

[https://www.jianshu.com/p/439f0e48618e](https://www.jianshu.com/p/439f0e48618e)

[https://blog.csdn.net/xijuezhu8128/article/details/86590562](https://blog.csdn.net/xijuezhu8128/article/details/86590562)

`torch.where(condition, x, y)`函数功能如下：

* 针对于x而言，如果其中的元素满足condition，就返回x的值；不满足condition的元素，将对应位置替换为y的值；
* 三个输入参数，第一个是判断条件，第二个是符合条件的设置值，第三个是不满足条件的设置值。

```python
In [2]: x = torch.randn(3, 2)                                   

In [3]: y = torch.ones(3, 2)                                    

In [4]: x                                                       
Out[4]: 
tensor([[-0.0106,  0.1663],
        [-0.6484, -0.3316],
        [ 0.4814, -1.9186]])

In [5]: y                                                       
Out[5]: 
tensor([[1., 1.],
        [1., 1.],
        [1., 1.]])

In [6]: torch.where(x > 0, x, y)                                
Out[6]: 
tensor([[1.0000, 0.1663],
        [1.0000, 1.0000],
        [0.4814, 1.0000]])
```

换种方式：

```python
In [2]: x = torch.randn(3, 2)                                   

In [3]: x                                                       
Out[3]: 
tensor([[ 0.1856,  1.1246],
        [-0.7023, -0.2031],
        [ 0.9781, -0.4329]])

In [4]: y = torch.tensor(3.0)                                   

In [5]: y                                                       
Out[5]: tensor(3.)

In [6]: torch.where(x > 0.5, x, y)                              
Out[6]: 
tensor([[3.0000, 1.1246],
        [3.0000, 3.0000],
        [0.9781, 3.0000]])
```

(7) **torch.nonzero()**

`torch.nonzero()`返回非零值的索引。

```python
In [2]: torch.nonzero(torch.tensor([1., 1., 1., 0., 1.]))       
Out[2]: 
tensor([[0],
        [1],
        [2],
        [4]])
```

第一、二、三、五个元素是1非零，所以返回的索引是0、1、2、4。

对于高维度张量：

```python
In [3]: torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0], 
   ...:                             [0.0, 0.4, 0.0, 0.0], 
   ...:                             [0.0, 0.0, 1.2, 0.0], 
   ...:                             [0.0, 0.0, 0.0,-0.4]]))     
Out[3]: 
tensor([[0, 0],
        [1, 1],
        [2, 2],
        [3, 3]])
```

##### 4.2 点对点操作

###### 4.2.1 三角函数

```python
In [3]: torch.abs(torch.tensor([-1.2, -0.23, 1.34]))            
Out[3]: tensor([1.2000, 0.2300, 1.3400])

In [4]: torch.acos(torch.tensor(0.5))                           
Out[4]: tensor(1.0472)

In [5]: torch.asin(torch.tensor(0.5))                           
Out[5]: tensor(0.5236)

In [6]: torch.atan(torch.tensor(1.))                            
Out[6]: tensor(0.7854)

In [7]: torch.atan2(torch.tensor(1.7321), torch.tensor(2))      
Out[7]: tensor(0.7137)

In [9]: torch.cos(torch.tensor(1.0472))                         
Out[9]: tensor(0.5000)

In [10]: torch.cosh(torch.tensor(1.2))                          
Out[10]: tensor(1.8107)

In [11]: torch.sin(torch.tensor(0.5236))                        
Out[11]: tensor(0.5000)

In [12]: torch.sinh(torch.tensor(1.2))                          
Out[12]: tensor(1.5095)

In [13]: torch.tan(torch.tensor(1.0472))                        
Out[13]: tensor(1.7321)

In [14]: torch.tanh(torch.tensor(1.2))                          
Out[14]: tensor(0.8337)
```

以下关于atan2函数的学习参考：[https://baike.baidu.com/item/atan2/10931300](https://baike.baidu.com/item/atan2/10931300)

`atan2(a, b)` 与 `atan(a, b)`稍有不同，**ATAN2(a/b)**的取值范围介于 -𝜋到𝜋之间（不包括 -𝜋），而**ATAN(a/b)**的取值范围介于-𝜋/2到𝜋/2之间（不包括±𝜋/2)。

另外要注意的是，函数`atan2(y, x)`中参数的顺序是倒置的，`atan2(y, x)`计算的值相当于点`(x, y)`的角度值。

###### 4.2.2 加减乘除

参考：[https://www.jb51.net/article/177464.htm](https://www.jb51.net/article/177464.htm)

加减：

```python
In [2]: a = torch.randn(3, 4)                                   

In [3]: a                                                       
Out[3]: 
tensor([[-0.5624,  0.5580,  0.9760,  0.1236],
        [-0.0491, -0.4877,  0.3196, -1.5708],
        [-0.8361,  1.0775, -0.0376,  0.2628]])

In [4]: b = torch.rand(4)                                       

In [5]: b                                                       
Out[5]: tensor([0.4848, 0.8852, 0.9253, 0.6578])

In [6]: torch.add(a, b)                                         
Out[6]: 
tensor([[-0.0776,  1.4432,  1.9012,  0.7814],
        [ 0.4356,  0.3975,  1.2449, -0.9130],
        [-0.3514,  1.9627,  0.8877,  0.9205]])

In [9]: torch.sub(a, b)                                         
Out[9]: 
tensor([[-1.0472, -0.3272,  0.0507, -0.5342],
        [-0.5339, -1.3729, -0.6057, -2.2286],
        [-1.3209,  0.1923, -0.9629, -0.3950]])
```

点乘：

```python
In [11]: a                                                      
Out[11]: 
tensor([[-0.5624,  0.5580,  0.9760,  0.1236],
        [-0.0491, -0.4877,  0.3196, -1.5708],
        [-0.8361,  1.0775, -0.0376,  0.2628]])

In [12]: b                                                      
Out[12]: tensor([0.4848, 0.8852, 0.9253, 0.6578])

In [13]: torch.mul(a, b)                                        
Out[13]: 
tensor([[-0.2726,  0.4939,  0.9030,  0.0813],
        [-0.0238, -0.4317,  0.2957, -1.0333],
        [-0.4053,  0.9538, -0.0348,  0.1728]])
```

点除：

```python
In [14]: a                                                      
Out[14]: 
tensor([[-0.5624,  0.5580,  0.9760,  0.1236],
        [-0.0491, -0.4877,  0.3196, -1.5708],
        [-0.8361,  1.0775, -0.0376,  0.2628]])

In [15]: b                                                      
Out[15]: tensor([0.4848, 0.8852, 0.9253, 0.6578])

In [16]: torch.div(a, b)                                        
Out[16]: 
tensor([[-1.1601,  0.6304,  1.0548,  0.1880],
        [-0.1014, -0.5510,  0.3454, -2.3881],
        [-1.7248,  1.2172, -0.0406,  0.3994]])
```

矩阵相乘：

包括`torch.mm()`、`torch.matmul()`、`@`，效果一样。

```python
In [4]: a                                                       
Out[4]: 
tensor([[-0.5178],
        [ 1.0223],
        [-0.3267]])

In [5]: b                                                       
Out[5]: tensor([[ 0.4189, -0.3158]])

In [6]: torch.mm(a, b)                                          
Out[6]: 
tensor([[-0.2169,  0.1635],
        [ 0.4282, -0.3229],
        [-0.1368,  0.1032]])

In [7]: torch.matmul(a, b)                                      
Out[7]: 
tensor([[-0.2169,  0.1635],
        [ 0.4282, -0.3229],
        [-0.1368,  0.1032]])

In [8]: a @ b                                                   
Out[8]: 
tensor([[-0.2169,  0.1635],
        [ 0.4282, -0.3229],
        [-0.1368,  0.1032]])
```

###### 4.2.3 对数运算

参考：[https://zhuanlan.zhihu.com/p/36233589](https://zhuanlan.zhihu.com/p/36233589)

```python
torch.log(input, out=None)    # y_i=log_e(x_i)
torch.log1p(input, out=None)  # y_i=log_e(x_i+1)
torch.log2(input, out=None)   # y_i=log_2(x_i)
torch.log10(input,out=None)   # y_i=log_10(x_i)
```

`torch.log()`实例如下：

```python
In [3]: torch.log(torch.tensor(2.71828))                        
Out[3]: tensor(1.0000)
```

复杂点的：

```
In [7]: epow = torch.randn(5, 4)                                

In [8]: epow                                                    
Out[8]: 
tensor([[-0.2526, -0.8183,  0.4722, -0.2324],
        [ 2.3240,  1.5123, -0.9960,  0.8525],
        [-0.0959, -0.1117,  1.5085, -0.6967],
        [ 1.7561, -0.1698, -1.4869, -0.1510],
        [ 0.8032, -1.6319,  0.9018, -0.6462]])

In [9]: a = torch.pow(torch.tensor(2.71828), epow)              

In [10]: a                                                      
Out[10]: 
tensor([[ 0.7768,  0.4412,  1.6036,  0.7926],
        [10.2164,  4.5373,  0.3694,  2.3455],
        [ 0.9086,  0.8943,  4.5198,  0.4982],
        [ 5.7896,  0.8439,  0.2261,  0.8598],
        [ 2.2328,  0.1956,  2.4640,  0.5240]])

In [11]: torch.log(a)                                           
Out[11]: 
tensor([[-0.2526, -0.8183,  0.4722, -0.2324],
        [ 2.3240,  1.5123, -0.9960,  0.8525],
        [-0.0959, -0.1117,  1.5085, -0.6967],
        [ 1.7561, -0.1698, -1.4869, -0.1510],
        [ 0.8032, -1.6319,  0.9018, -0.6462]])
```

`torch.log1p()`实例如下：

```python
In [16]: torch.log1p(torch.tensor(1.71828))  # y_i=log_e(x_i+1)，用2.71828 - 1
Out[16]: tensor(1.0000)
```

`torch.log2()`实例如下：

```python
In [18]: torch.log2(torch.tensor(8.))                           
Out[18]: tensor(3.)
```

`torch.log10()`实例如下：

```python
In [19]: torch.log10(torch.tensor(1000.))                       
Out[19]: tensor(3.)
```

###### 4.2.4 幂函数

```python
torch.pow(input, exponent, out=None)  # y_i=input^(exponent)
```

实例如下：

```python
In [2]: torch.pow(torch.tensor(2), 3)                           
Out[2]: tensor(8)

In [3]: torch.pow(torch.randn(2, 3), 3)                         
Out[3]: 
tensor([[-0.0034, -0.0333,  0.0436],
        [-0.1628,  0.0019, -0.0073]])
```

###### 4.2.5 指数运算

```python
torch.exp(tensor, out=None)     # y_i=e^(x_i)
torch.expm1(tensor, out=None)   # y_i=e^(x_i) -1
```

实例如下：

```python
In [6]: torch.exp(torch.tensor(0.))                             
Out[6]: tensor(1.)

In [7]: torch.expm1(torch.tensor(0.))                           
Out[7]: tensor(0.)
```

###### 4.2.6 截断函数

```python
torch.ceil(input, out=None)   						# 返回向正方向取得最小整数
torch.floor(input, out=None)  						# 返回向负方向取得最大整数

torch.round(input, out=None)  						# 返回相邻最近的整数，四舍五入

torch.trunc(input, out=None)  						# 返回整数部分数值
torch.frac(tensor, out=None)  						# 返回小数部分数值

torch.fmod(input, divisor, out=None)  		# 返回input/divisor的余数
torch.remainder(input, divisor, out=None) # 同上
```

###### 4.2.7 其他运算

```python
torch.erf(tensor， out=None)
torch.erfinv(tensor, out=None)
torch.sigmoid(input, out=None)
torch.clamp(input, min, max, out=None) # 返回input<min,则返回min，input>max,则返回max,其余返回input
torch.neg(input, out=None) 						 # out_i=-1*(input)
torch.reciprocal(input, out=None)  		 # out_i= 1/input_i
torch.sqrt(input, out=None)  					 # out_i=sqrt(input_i)
torch.rsqrt(input, out=None)					 # out_i=1/(sqrt(input_i))
torch.sign(input, out=None)  					 # out_i=sin(input_i)  大于0为1，小于0为-1
torch.lerp(start, end, weight, out=None)
```

##### 4.3 降维操作

```python
torch.argmax(input, dim=None, keepdim=False)  # 返回最大值排序的索引值
torch.argmin(input, dim=None, keepdim=False)  # 返回最小值排序的索引值

torch.cumprod(input, dim, out=None)  					# y_i=x_1 * x_2 * x_3 *…* x_i
torch.cumsum(input, dim, out=None)  					# y_i=x_1 + x_2 + … + x_i

torch.dist(input, out, p=2)       						# 返回input和out的p式距离
torch.mean()                     						  # 返回平均值
torch.sum()                      						  # 返回总和
torch.median(input)               						# 返回中间值
torch.mode(input)                						  # 返回众数值
torch.unique(input, sorted=False) 						# 返回1-D的唯一的tensor,每个数值返回一次.
>>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
>>> output
tensor([ 2,  3,  1])

torch.std()  																	# 返回标准差
torch.var() 																	# 返回方差

torch.norm(input, p=2) 												# 返回p-norm的范式
torch.prod(input, dim, keepdim=False) 				# 返回指定维度每一行的乘积
```

##### 4.4 对比操作

```python
torch.eq(input, other, out=None)  		# 按成员进行等式操作，相同返回1
torch.equal(tensor1, tensor2) 				# 如果tensor1和tensor2有相同的size和elements，则为true
>>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[ 1,  0],
        [ 0,  1]], dtype=torch.uint8)

torch.ge(input, other, out=None)   		# input>= other
torch.gt(input, other, out=None)   		# input>other
torch.le(input, other, out=None)    	# input=<other
torch.lt(input, other, out=None)   	  # input<other
torch.ne(input, other, out=None)  		# input != other 不等于

torch.max()                       	  # 返回最大值
torch.min()                       	  # 返回最小值
torch.isnan(tensor) 									# 判断是否为’nan’
torch.sort(input, dim=None, descending=False, out=None) # 对目标input进行排序
torch.topk(input, k, dim=None, largest=True, sorted=True, out=None)  # 沿着指定维度返回最大k个数值及其索引值
torch.kthvalue(input, k, dim=None, deepdim=False, out=None) # 沿着指定维度返回最小k个数值及其索引值
```

##### 4.5 频谱操作

目前不大懂，先抄下来留存。

```python
torch.fft(input, signal_ndim, normalized=False)
torch.ifft(input, signal_ndim, normalized=False)
torch.rfft(input, signal_ndim, normalized=False, onesided=True)
torch.irfft(input, signal_ndim, normalized=False, onesided=True)
torch.stft(signa, frame_length, hop, …)
```

##### 4.6 其他操作

```python
torch.cross(input, other, dim=-1, out=None)  						# 叉乘(外积)
torch.dot(tensor1, tensor2)  														# 返回tensor1和tensor2的点乘
torch.mm(mat1, mat2, out=None) 													# 返回矩阵mat1和mat2的乘积
torch.eig(a, eigenvectors=False, out=None) 							# 返回矩阵a的特征值/特征向量 
torch.det(A)  																					# 返回矩阵A的行列式
torch.trace(input) 																			# 返回2-d矩阵的迹(对对角元素求和)
torch.diag(input, diagonal=0, out=None)									# 返回对角线上的元素
torch.histc(input, bins=100, min=0, max=0, out=None) 		# 计算input的直方图
torch.tril(input, diagonal=0, out=None)  								# 返回矩阵的下三角矩阵，其他为0
torch.triu(input, diagonal=0, out=None) 								# 返回矩阵的上三角矩阵，其他为0
```

#### 5. Tips

##### 5.1 获取Python number

```python
In [3]: a                                                       
Out[3]: 
tensor([[-0.7424, -0.7418,  0.3529],
        [-0.5532,  0.0865, -0.7361],
        [ 0.5913, -0.7325, -0.3438],
        [-0.3184, -0.7912, -1.2510],
        [-0.3555, -0.2875,  0.9544]])

In [4]: a[0]  # 直接取索引返回的是tensor数据
Out[4]: tensor([-0.7424, -0.7418,  0.3529])

In [5]: a[0].item()                                             
----------------------------------------------------------------
ValueError                     Traceback (most recent call last)
<ipython-input-5-b8dd84f0b77f> in <module>
----> 1 a[0].item()

ValueError: only one element tensors can be converted to Python scalars

In [6]: a[0][1].item() # 获取python number，具体数据
Out[6]: -0.74179607629776
```

##### 5.2 tensor设置

###### 5.2.1 判断类型

```python
torch.is_tensor()  # 如果是pytorch的tensor类型返回true
torch.is_storage() # 如果是pytorch的storage类型返回ture
```

`torch.Storage()`

参考： 

[https://www.cnblogs.com/wanghui-garcia/p/10623033.html](https://www.cnblogs.com/wanghui-garcia/p/10623033.html)

[https://blog.csdn.net/Mr_JP/article/details/81906616](https://blog.csdn.net/Mr_JP/article/details/81906616)

tensor分为头信息区（Tensor）和存储区（Storage）。

信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组，存储在存储区。

```python
In [2]: a = torch.FloatTensor([1, 2, 3])                        

In [3]: a                                                       
Out[3]: tensor([1., 2., 3.])

In [4]: b = torch.FloatStorage([1, 2, 3])                       

In [5]: b                                                       
Out[5]: 
 1.0
 2.0
 3.0
[torch.FloatStorage of size 3]

In [6]: a.dtype, b.dtype                                        
Out[6]: (torch.float32, torch.float32)

In [7]: torch.is_tensor(a)                                      
Out[7]: True

In [8]: torch.is_storage(b)                                     
Out[8]: True
```

###### 5.2.2 设置默认

以下留存吧，用的比较少。

```python
torch.set_default_dtype(d)  					# 对torch.tensor() 设置默认的浮点类型

torch.set_default_tensor_type() 			# 同上，对torch.tensor()设置默认的tensor类型
>>> torch.tensor([1.2, 3]).dtype      # initial default for floating point is torch.float32
torch.float32
>>> torch.set_default_dtype(torch.float64)
>>> torch.tensor([1.2, 3]).dtype      # a new floating point tensor
torch.float64
>>> torch.set_default_tensor_type(torch.DoubleTensor)
>>> torch.tensor([1.2, 3]).dtype    	# a new floating point tensor
torch.float64

torch.get_default_dtype() 						# 获得当前默认的浮点类型torch.dtype
torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None）												# 设置printing的打印参数
```





#### References

* [pytorch入坑一: Tensor及其基本操作](https://zhuanlan.zhihu.com/p/36233589)
* [Pytorch中.new()的作用详解](https://www.jb51.net/article/180679.htm)
* [浅谈pytorch和Numpy的区别以及相互转换方法](https://www.jb51.net/article/144520.htm)
* [**torch.eye**](https://www.cnblogs.com/btschang/p/10300727.html)
* [PyTorch的学习笔记](https://www.cnblogs.com/gato-chat/p/9064974.html)
* [pytorch.range() 和 pytorch.arange() 的区别](https://blog.csdn.net/m0_37586991/article/details/88830026)
* [PyTorch中linspace的详细用法](https://blog.csdn.net/york1996/article/details/81671128)
* [torch.normal()](https://blog.csdn.net/sxs11/article/details/81775715)
* [torch.randn和torch.rand有什么区别](https://blog.csdn.net/wangwangstone/article/details/89815661)
* [randperm函数](https://www.cnblogs.com/ymjyqsx/p/6200692.html)
* [Pytorch中index_select() 函数的实现理解](https://www.jb51.net/article/174524.htm)
* [torch.masked_select 掩码](https://blog.csdn.net/q511951451/article/details/81611903)
* [Pytorch mask_select 函数](https://blog.csdn.net/SoftPoeter/article/details/81667810)
* [Pytorch中的torch.cat()函数](https://www.cnblogs.com/JeasonIsCoding/p/10162356.html)
* [torch.stack()的使用](https://blog.csdn.net/Teeyohuang/article/details/80362756)
* [pytorch中的cat、stack、tranpose、permute、unsqeeze](https://www.cnblogs.com/yifdu25/p/9399047.html)
* [pytorch之torch.gather方法](https://blog.csdn.net/edogawachia/article/details/80515038)
* [pytorch中的gather函数](https://blog.csdn.net/edogawachia/article/details/80515038)
* [torch.split() 与 torch.chunk()](https://www.codetd.com/article/8938026)
* [pytorch学习 中 torch.squeeze() 和torch.unsqueeze()的用法](https://blog.csdn.net/xiexu911/article/details/80820028)
* [torch.t()](https://blog.csdn.net/foneone/article/details/103876131)
* [Pytorch阅读文档之reshape，view，flatten, transpose函数](https://blog.csdn.net/GhostintheCode/article/details/102531237)
* [Pytorch-Where](https://www.jianshu.com/p/439f0e48618e)
* [np.where和torch.where的使用区别](https://blog.csdn.net/xijuezhu8128/article/details/86590562)
* [atan2](https://baike.baidu.com/item/atan2/10931300)
* [Pytorch Tensor基本数学运算详解](https://www.jb51.net/article/177464.htm)
* [torch.Storage](https://blog.csdn.net/Mr_JP/article/details/81906616)
* [pytorch torch.Storage学习](https://www.cnblogs.com/wanghui-garcia/p/10623033.html)
