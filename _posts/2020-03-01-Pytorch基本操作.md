---
title: Pytorch基本操作
author: 赵旭山
tags: Pytorch
---

#### 1. Pytorch基本操作概要

搜索网页学习Pytorch中看到一张结构清晰的思维导图，作者总结得很清晰，引用过来学习学习。如下：

Figure source: [https://zhuanlan.zhihu.com/p/36233589](https://zhuanlan.zhihu.com/p/36233589)

![](/assets/images/pytorchflowchart202002272158.jpg)

#### 2. Tensor属性

```python
x = torch.rand(5, 3)
x.dtype
x.device
x.layout
x.stride()
```

> torch.float32
>
> device(type='cpu')
>
> torch.strided
>
> (3, 1)

dtype就是数据的类型，Pytorch共有八种数据类型：

| Data type                | dtype                         | dtype                |
| ------------------------ | ----------------------------- | -------------------- |
| 32-bit floating point    | torch.float32 or torch.float  | torch.*.FloatTensor  |
| 64-bit floating point    | torch.float64 or torch.double | torch.*.DoubleTensor |
| 16-bit floating point    | torch.float16 or torch.half   | torch.*.HalfTensor   |
| 8-bit integer (unsigned) | torch.uint8                   | torch.*.ByteTensor   |
| 8-bit integer (signed)   | torch.int8                    | torch.*.CharTensor   |
| 16-bit integer (signed)  | torch.int16 or torch.short    | torch.*.ShortTensor  |
| 32-bit integer (signed)  | torch.int32 or torch.int      | torch.*.IntTensor    |
| 64-bit integer (signed)  | torch.int64 or torch.long     | torch.*.LongTensor   |

layout是数据在内存中是怎么存的，分为稀疏张量和致密张量，常用的为致密张量（`torch.strided`）。

笔者**朴素的理解**就是定义的变量在内存中咋存储的，对二维矩阵而言，用了多少列，行则被展平存成一行。`x.t()`等同于矩阵转置，没存，只把两个数据交换了一下输出。

```python
In [7]: x.stride()                                              
Out[7]: (3, 1)

In [8]: x                                                       
Out[8]: 
tensor([[0.6031, 0.0937, 0.3282],
        [0.2758, 0.8520, 0.1412],
        [0.9676, 0.9618, 0.5699],
        [0.1657, 0.2150, 0.9051],
        [0.7828, 0.9131, 0.4832]])

In [9]: x.stride()                                              
Out[9]: (3, 1)

In [10]: x.t()                                                  
Out[10]: 
tensor([[0.6031, 0.2758, 0.9676, 0.1657, 0.7828],
        [0.0937, 0.8520, 0.9618, 0.2150, 0.9131],
        [0.3282, 0.1412, 0.5699, 0.9051, 0.4832]])

In [11]: x.t().stride()                                         
Out[11]: (1, 3)
```

对于三维张量，后两维相乘是第一个数，最后一维是第二个数，最后一个数为1。t()函数只适用于2维及以下的张量，三维及以上不适用。

```python
In [30]: x =torch.rand(3, 8, 5)                                 

In [31]: x.size()                                               
Out[31]: torch.Size([3, 8, 5])

In [32]: x.stride()                                             
Out[32]: (40, 5, 1)

In [33]: xx = torch.rand(6, 3, 7)                               

In [34]: xx.size()                                              
Out[34]: torch.Size([6, 3, 7])

In [35]: xx.stride()                                            
Out[35]: (21, 7, 1)
  
In [36]: xx.t()                                                 
----------------------------------------------------------------
RuntimeError                   Traceback (most recent call last)
<ipython-input-36-0b88b5bf88e4> in <module>
----> 1 xx.t()

RuntimeError: t() expects a tensor with <= 2 dimensions, but self is 3D

```

#### 3. 创建Tensor

##### 3.1 直接创建

**torch.tensor()**

```python
In [2]: cell = torch.tensor([[6.50, 0, 0], [0, 6.50, 0], [0, 0, 
   ...: 6.50]])                                                 

In [3]: cell                                                    
Out[3]: 
tensor([[6.5000, 0.0000, 0.0000],
        [0.0000, 6.5000, 0.0000],
        [0.0000, 0.0000, 6.5000]])
```

##### 3.2 创建类型相似但size不同的tensor

**.new()**：`inputs.new`&`torch.Tensor.new`

参考: [https://www.jb51.net/article/180679.htm](https://www.jb51.net/article/180679.htm)

创建一个新的Tensor，该Tensor的**type**和**device**都和原有Tensor一致，且无内容。

```python
In [2]: inputs = torch.rand(5, 3)                               

In [3]: inputs                                                  
Out[3]: 
tensor([[0.7991, 0.8743, 0.1387],
        [0.9594, 0.9258, 0.3077],
        [0.3318, 0.3850, 0.5850],
        [0.9942, 0.3455, 0.2891],
        [0.4658, 0.9288, 0.7334]])

In [4]: new_inputs1 = inputs.new()                               

In [5]: new_inputs1                                              
Out[5]: tensor([])

In [6]: new_inputs2 = torch.Tensor.new(inputs)                  

In [7]: new_inputs2                                             
Out[7]: tensor([])

In [8]: new_inputs3 = inputs.new(inputs.size())                 

In [9]: new_inputs3                                            
Out[9]: 
tensor([[0.0000e+00, 1.4013e-45, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00],
        [1.1704e-41, 0.0000e+00, 2.2369e+08],
        [0.0000e+00, 0.0000e+00, 0.0000e+00]])

In [10]: inputs.dtype, new_inputs1.dtype, new_inputs2.dtype, new_inputs3.dtype                                          
Out[10]: (torch.float32, torch.float32, torch.float32, torch.float32)

In [11]: inputs.device, new_inputs.device, new_inputs2.device, new_inputs3.device                                      
Out[11]: 
(device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu'))
```

实际应用：**添加噪声**

可以对Tensor**添加噪声**，添加如下代码即可实现：

```python
In [17]: noise = inputs.new(inputs.size()).normal_(0, 0.01)    

In [18]: noise                                                 
Out[18]: 
tensor([[-0.0021, -0.0025,  0.0182],
        [-0.0016,  0.0106, -0.0039],
        [ 0.0023,  0.0112,  0.0024],
        [ 0.0109,  0.0007,  0.0089],
        [ 0.0177,  0.0003, -0.0026]])
```

类似的有：`.new_ones`、`.new_zeros`。

##### 3.3 从numpy生成

**torch.from_numpy()**

参考：https://www.jb51.net/article/144520.htm

```python
In [3]: np_data = np.arange(6).reshape((2, 3))                  

In [4]: np_data                                                 
Out[4]: 
array([[0, 1, 2],
       [3, 4, 5]])

# numpy 转为 pytorch格式

In [5]: torch_data = torch.from_numpy(np_data)                  

In [6]: torch_data                                              
Out[6]: 
tensor([[0, 1, 2],
        [3, 4, 5]])

# torch 转为numpy

In [7]: tensor2array = torch_data.numpy()                       

In [8]: tensor2array                                            
Out[8]: 
array([[0, 1, 2],
       [3, 4, 5]])
```

矩阵转换为Tensor：

```python
# 转为32位浮点数，torch接受的都是Tensor的形式
In [1]: data = [[1, 2], [3, 4]]                                 

In [2]: data                                                    
Out[2]: [[1, 2], [3, 4]]

In [3]: tensor = torch.FloatTensor(data)                        

In [4]: tensor                                                  
Out[4]: 
tensor([[1., 2.],
        [3., 4.]])
```

##### 3.4 创建特定Tensor

###### 3.4.1 根据矩阵要求

(1) **torch.eye()**

参考：https://www.cnblogs.com/btschang/p/10300727.html

对角线位置全1，其它位置全为0的二维矩阵。

```
In [7]: torch.eye(6)                                            
Out[7]: 
tensor([[1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1.]])
```

(2) **torch.empty()**

参考：https://www.cnblogs.com/gato-chat/p/9064974.html

empty方法创建的矩阵不是空矩阵，而是未初始化的矩阵，所以里面的值不一定是0。

```python
In [2]: x = torch.empty(5, 3)                                   

In [3]: x                                                       
Out[3]: 
tensor([[0.0000e+00, -0.0000e+00, 0.0000e+00],
        [-0.0000e+00, 1.8361e+25, 1.4603e-19],
        [1.6795e+08, 4.7423e+30, 4.7393e+30],
        [9.5461e-01, 4.4377e+27, 1.7975e+19],
        [4.6894e+27, 7.9463e+08, 3.2604e-12]])
```

(3) **torch.empty_like()**

`torch.empty_like(input)`返回和input的tensor一样size的empty张量。

```python
In [2]: x = torch.rand(5, 3)                                    

In [3]: x                                                       
Out[3]: 
tensor([[0.9643, 0.6685, 0.7706],
        [0.4562, 0.1438, 0.7719],
        [0.3813, 0.3628, 0.9026],
        [0.3653, 0.0948, 0.2917],
        [0.6953, 0.5089, 0.1560]])

In [4]: xx = torch.empty_like(x)                                

In [5]: xx                                                      
Out[5]: 
tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 1.8028e+28, 3.4740e-12],
        [1.4583e-19, 7.3470e+28, 6.1425e+28],
        [7.1441e+31, 6.9987e+22, 7.8675e+34],
        [4.7418e+30, 5.9663e-02, 7.0374e+22]])
```

###### 3.4.2 根据数值要求

(1) **torch.zeros()**

参考：https://www.cnblogs.com/gato-chat/p/9064974.html

生成0矩阵，detype参数指定了生成的数据的类型。

```python
In [9]: x = torch.zeros(5, 3, dtype=torch.long)                 

In [10]: x                                                      
Out[10]: 
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
```

(2) **torch.zeros_like()**

`torch.zeros_like(input)`返回跟input的tensor一个size的全零tensor。

```python
In [6]: xx = torch.zeros_like(x)                                

In [7]: xx                                                      
Out[7]: 
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
```

以下的`*_like`方法与以上相同，不再一一赘述。

(3) **torch.ones()**

返回全1矩阵。

```python
In [2]: x = torch.ones(5, 3)                                    

In [3]: x                                                       
Out[3]: 
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
```

(4) **torch.full()**

`torch.full(size, fill_value)`生成一个用fill_value填充的特定size张量。

```python
In [6]: x = torch.full((5, 3), 2.71828)                         

In [7]: x                                                       
Out[7]: 
tensor([[2.7183, 2.7183, 2.7183],
        [2.7183, 2.7183, 2.7183],
        [2.7183, 2.7183, 2.7183],
        [2.7183, 2.7183, 2.7183],
        [2.7183, 2.7183, 2.7183]])
```

(5) **torch.arange()** & **torch.range()**

参考：https://blog.csdn.net/m0_37586991/article/details/88830026

```python
In [2]: x = torch.range(1, 6)                                   
/usr/local/Caskroom/miniconda/base/bin/ipython:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].
  #!/usr/local/Caskroom/miniconda/base/bin/python

In [3]: x                                                       
Out[3]: tensor([1., 2., 3., 4., 5., 6.])

In [4]: y = torch.arange(1, 6)                                  

In [5]: y                                                       
Out[5]: tensor([1, 2, 3, 4, 5])

In [6]: x.dtype, y.dtype                                        
Out[6]: (torch.float32, torch.int64)
```

*  `torch.range(start=1, end=6)`的结果是会包含`end`的，而`torch.arange(start=1, end=6)`的结果并不包括`end`；

* 两者创建的`tensor`的类型也不一样。

(6) **torch.linspace()**

参考：https://blog.csdn.net/york1996/article/details/81671128

`torch.linspace(start, end, steps)`返回一个一维的tensor，这个张量包含了从start到end，分成steps个段得到的向量。输出张量的长度由steps决定。

```python
In [2]: torch.linspace(3, 10, 5)                                
Out[2]: tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])

In [3]: torch.linspace(-10, 10, steps=6)                        
Out[3]: tensor([-10.,  -6.,  -2.,   2.,   6.,  10.])
```

(7) **torch.logspace()**

`torch.logspace(start, end, steps)`返回一个一维的tensor，这个张量包含了从<img src="http://latex.codecogs.com/gif.latex?\10^{start}" />  到<img src="http://latex.codecogs.com/gif.latex?\10^{end}" />，分成steps个段得到的向量。输出张量的长度由steps决定。

```python
In [4]: torch.logspace(1, 5, 4)                                 
Out[4]: tensor([1.0000e+01, 2.1544e+02, 4.6416e+03, 1.0000e+05])

In [5]: torch.logspace(1, 5, 5)                                 
Out[5]: tensor([1.0000e+01, 1.0000e+02, 1.0000e+03, 1.0000e+04, 1.0000e+05])
```

##### 3.5 随机采样生成

(1) **torch.normal()**

参考：[https://blog.csdn.net/sxs11/article/details/81775715](https://blog.csdn.net/sxs11/article/details/81775715)

`torch.normal(mean, std)`返回一个张量，包含从给定参数means、std的离散正态分布中抽取随机数。

> mean — 均值
>
> std — 标准差

```python
In [11]: torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))                                                    
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-11-4540d51055de> in <module>
----> 1 torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))

TypeError: normal() received an invalid combination of arguments - got (std=Tensor, means=Tensor, ), but expected one of:
 * (Tensor mean, Tensor std, torch.Generator generator, Tensor out)
 * (Tensor mean, float std, torch.Generator generator, Tensor out)
 * (float mean, Tensor std, torch.Generator generator, Tensor out)
 * (float mean, float std, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)


In [12]: torch.normal(mean=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))                                                     
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-12-a4a4388458ef> in <module>
----> 1 torch.normal(mean=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))

RuntimeError: "norma_cpu" not implemented for 'Long'

In [13]: torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))                                                   
Out[13]: 
tensor([ 1.8979,  1.5922,  3.0055,  3.7566,  4.8068,  6.4627,  7.1539,  7.4512,
         9.1401, 10.0355])
```

简单分析`In [11]`、`In [12]`两种错误：

**In [11]**: [参考网页](https://blog.csdn.net/sxs11/article/details/81775715)中输入参数`means=...`不工作，改成`mean=...`就可以了，另外此处给出了`normal`函数的几种输入组合：

>  * (Tensor mean, Tensor std, torch.Generator generator, Tensor out)
>  * (Tensor mean, float std, torch.Generator generator, Tensor out)
>  * (float mean, Tensor std, torch.Generator generator, Tensor out)
>  * (float mean, float std, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)

**In [12]**: 根据上述输入组合，`std`和`mean`只接受`float`和`Tensor`两种dtype，所以`In [12]`中`mean=torch.arange(1, 11)`不工作，改成`mean=torch.arange(1., 11.)`即可。

其中：

```python
In [13]: torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))                                                   
Out[13]: 
tensor([ 1.8979,  1.5922,  3.0055,  3.7566,  4.8068,  6.4627,  7.1539,  7.4512,
         9.1401, 10.0355])
```

根据`mean`和`std`生成的张量含义如下：

```python
 1.8979 #是从均值为1，标准差为1的正态分布中随机生成的
 1.5922 #是从均值为2，标准差为0.9的正态分布中随机生成的
 3.0055 #是从均值为3，标准差为0.8的正态分布中随机生成的
 3.7566 #是从均值为4，标准差为0.7的正态分布中随机生成的
 4.8068 #是从均值为5，标准差为0.6的正态分布中随机生成的
 6.4627 #是从均值为6，标准差为0.5的正态分布中随机生成的
 7.1539 #是从均值为7，标准差为0.4的正态分布中随机生成的
 7.4512 #是从均值为8，标准差为0.3的正态分布中随机生成的
 9.1401 #是从均值为9，标准差为0.2的正态分布中随机生成的
10.0355 #是从均值为10，标准差为0.1的正态分布中随机生成的
```

<u>6.4627（均值为6，标准差为0.5）和7.4512（是从均值为8，标准差为0.3）</u>两个值有所偏差。

(2) **torch.rand()** & **torch.randn()**

torch.randn()和torch.rand()有什么区别

参考：[https://blog.csdn.net/wangwangstone/article/details/89815661](https://blog.csdn.net/wangwangstone/article/details/89815661)

torch.rand和torch.randn有什么区别？ y = torch.rand(5,3) y=torch.randn(5,3)

一个均匀分布，一个是标准正态分布。

***均匀分布***

`torch.rand(*size)`返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数。张量的形状由参数size定义。

```python
In [21]: torch.rand(size=(3, 2))                                 
Out[21]: 
tensor([[0.2218, 0.1115],
        [0.7232, 0.4805],
        [0.7677, 0.1653]])

In [22]: torch.rand(5, 3)                                       
Out[22]: 
tensor([[0.4548, 0.8788, 0.6698],
        [0.6992, 0.2677, 0.1843],
        [0.8135, 0.6389, 0.9419],
        [0.8074, 0.5690, 0.4269],
        [0.7801, 0.3185, 0.3531]])
```

***标准正态分布***

`torch.randn(*size)`返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）。

```python
In [23]: torch.randn(size=(4, 2))                               
Out[23]: 
tensor([[-2.5096,  0.3663],
        [ 0.8938,  0.3082],
        [ 0.0469,  0.3610],
        [-0.6472, -0.0929]])

In [24]: torch.randn(4, 2)                                      
Out[24]: 
tensor([[ 0.1326,  1.4875],
        [-0.3001,  0.0889],
        [ 0.5647,  0.5865],
        [ 0.6648,  0.7136]])
```

>`其他`：
>
>***离散正态分布***
>
>`torch.normal(mean, std)`，用法见前述。
>
>***线性间距向量***
>
>`torch.linspace(start, end, steps=100)`，用法见前述。

(3) **torch.randint()**

`torch.randint(low, high, size)`返回`low`～`high`之间整数特定`size`的张量。

```python
In [18]: idx = torch.randint(low=0, high=20, size=(10, 10))     

In [19]: idx                                                    
Out[19]: 
tensor([[ 6, 17, 14, 14, 14,  9,  6,  2,  2,  5],
        [19, 18, 13,  8, 14,  8,  1,  8, 17, 14],
        [ 9,  3,  1, 19,  2,  6,  1, 16,  8,  6],
        [18, 12,  0, 12,  6,  3, 14,  6, 17, 12],
        [10,  3,  9, 16,  7,  4,  5,  2, 19, 14],
        [ 3,  0, 13, 11,  0, 16,  3, 12,  7, 12],
        [ 1, 15, 15,  3, 15,  5, 13,  8,  7,  3],
        [ 8,  2, 11, 11, 13, 14,  5, 13,  0, 17],
        [ 0, 10, 15,  4, 12, 17, 19,  7, 13,  1],
        [16, 15, 12,  7, 15, 11,  8, 16, 11,  4]])

In [20]: idx.dtype                                              
Out[20]: torch.int64
```

`low`和`high`必须也是整数，否则会报错。

```python
In [21]: idx = torch.randint(low=0.5, high=20.1, size=(10, 10)) 
----------------------------------------------------------------
TypeError                      Traceback (most recent call last)
<ipython-input-21-94e2fb84d2eb> in <module>
----> 1 idx = torch.randint(low=0.5, high=20.1, size=(10, 10))

TypeError: randint() received an invalid combination of arguments - got (size=tuple, high=float, low=float, ), but expected one of:
 * (int high, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
 * (int low, int high, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
```

(4) **torch.randperm()**

参考：https://www.cnblogs.com/ymjyqsx/p/6200692.html

`torch.randperm(n)`功能是把1到n这些数随机打乱得到一个数字序列。

```python
In [22]: xrp = torch.randperm(10)                               

In [23]: xrp                                                    
Out[23]: tensor([4, 9, 2, 7, 1, 5, 6, 0, 3, 8])

In [24]: xrp.dtype                                              
Out[24]: torch.int64
```

#### 4. 操作Tensor

##### 4.1 基本操作

###### 4.1.1 indexing

(1) **torch.index_select()**

参考：[https://www.jb51.net/article/174524.htm](https://www.jb51.net/article/174524.htm)

`index_select(input, dim, index)`表示**从张量的某个维度的指定位置选取数据**。

`input`为输入的tensor，即索引的对象；`dim`表示从第几维挑选数据，0表示按行索引，1表示按列索引；`index`表示从第`dim`个参数维度中的哪个位置挑选数据。

```python
In [2]: x = torch.randint(0, 99, size=(10, 10))                 

In [3]: x                                                       
Out[3]: 
tensor([[45, 81, 16, 56, 95, 91, 37, 34, 81, 98],
        [94, 87,  7, 71, 53, 43, 86, 15, 62, 71],
        [57, 30, 78, 50, 68, 19, 12, 52,  0, 72],
        [84, 27, 44, 26, 79, 72, 32, 15, 73, 98],
        [14, 21, 40, 65, 76, 88, 89,  6, 71, 18],
        [67,  9, 27,  3, 50, 72, 56, 68, 18, 87],
        [37,  7,  5, 60,  6, 12, 17, 24, 83, 23],
        [32, 55, 79, 83, 55, 26, 44, 62, 22, 25],
        [ 3, 16, 47, 45, 18, 47, 46, 19, 16,  4],
        [50, 92, 86, 11, 90, 53,  1, 92, 22, 92]])

In [4]: torch.index_select(x, 0, 2)                             
----------------------------------------------------------------
TypeError                      Traceback (most recent call last)
<ipython-input-4-a9bfb26e8f43> in <module>
----> 1 torch.index_select(x, 0, 2)

TypeError: index_select() received an invalid combination of arguments - got (Tensor, int, int), but expected one of:
 * (Tensor input, name dim, Tensor index, Tensor out)
 * (Tensor input, int dim, Tensor index, Tensor out)

In [5]: torch.index_select(x, 0, torch.tensor([0, 2]))          
Out[5]: 
tensor([[45, 81, 16, 56, 95, 91, 37, 34, 81, 98],
        [57, 30, 78, 50, 68, 19, 12, 52,  0, 72]])

In [6]: torch.index_select(x, 0, torch.tensor([8]))                                                                               
Out[6]: tensor([[ 3, 16, 47, 45, 18, 47, 46, 19, 16,  4]])
  
In [7]: torch.index_select(x, 1, torch.tensor([0, 6]))                                                                           
Out[7]: 
tensor([[45, 37],
        [94, 86],
        [57, 12],
        [84, 32],
        [14, 89],
        [67, 56],
        [37, 17],
        [32, 44],
        [ 3, 46],
        [50,  1]])
```

从`In [4]`报错信息可知，index也须是一个Tensor：

>  * (Tensor input, name dim, Tensor index, Tensor out)
>  * (Tensor input, int dim, Tensor index, Tensor out)

dim = 0时，torch.tensor([0, 2])表示第0行和第2行，torch.tensor([8])表示第8行；

dim = 1时，torch.tensor([0, 6])表示第0行和第6行。

(2) **torch.masked_select()**

参考：[https://blog.csdn.net/q511951451/article/details/81611903](https://blog.csdn.net/q511951451/article/details/81611903)；[https://blog.csdn.net/SoftPoeter/article/details/81667810](https://blog.csdn.net/SoftPoeter/article/details/81667810)

`torch.masked_select(input, mask)`将根据张量mask中的二元值，取输入张量🀄️的指定项（mask为一个Byte Tensor），将取值返回到一个新的1D张量，**张量mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同**。返回的张量不与原始张量共享内存空间。

`input`(Tensor)为输入张量，`mask`(Byte Tensor)为掩码张量，包含了二元索引值。

```python
In [2]: x = torch.randn(5, 3)                                   

In [3]: x                                                       
Out[3]: 
tensor([[ 0.2893, -0.1957, -0.9152],
        [ 0.6943,  0.5319,  0.8884],
        [-0.8882,  0.5574,  0.7390],
        [ 1.4411,  0.3321, -0.9831],
        [ 0.9207,  0.8473, -0.4209]])

In [4]: mask = x.ge(0.5)                                        

In [5]: mask                                                    
Out[5]: 
tensor([[False, False, False],
        [ True,  True,  True],
        [False,  True,  True],
        [ True, False, False],
        [ True,  True, False]])

In [6]: torch.masked_select(x, mask)                            
Out[6]: tensor([0.6943, 0.5319, 0.8884, 0.5574, 0.7390, 1.4411, 0.9207, 0.8473])
```

###### 4.1.2 joining

(1) **torch.cat()**

参考：https://www.cnblogs.com/JeasonIsCoding/p/10162356.html

`torch.cat((tensor_A, tensor_B), dim)`表示把张量A、B拼接在一起。

***基本用法***：拼接两个Tensor

dim为0时表示按行拼接：

```python
In [2]: A = torch.ones(4, 3)                                    

In [3]: A                                                       
Out[3]: 
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])

In [4]: B = torch.zeros(2, 3)                                   

In [5]: B                                                       
Out[5]: 
tensor([[0., 0., 0.],
        [0., 0., 0.]])

In [6]: C = torch.cat((A, B), 0) # A，B须列数相同                               

In [7]: C                                                       
Out[7]: 
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [0., 0., 0.],
        [0., 0., 0.]])
```

dim为1时表示按列拼接：

```python
In [2]: A = torch.ones(4, 3)                                    

In [3]: A                                                       
Out[3]: 
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])

In [8]: AA = torch.ones(4, 2) * 2                               

In [9]: AA                                                      
Out[9]: 
tensor([[2., 2.],
        [2., 2.],
        [2., 2.],
        [2., 2.]])

In [10]: CC = torch.cat((A, AA), 1) #A，AA须行数相同                            

In [11]: CC                                                     
Out[11]: 
tensor([[1., 1., 1., 2., 2.],
        [1., 1., 1., 2., 2.],
        [1., 1., 1., 2., 2.],
        [1., 1., 1., 2., 2.]])
```

***其他用法***：拼接两个list

```python
In [2]: x = torch.Tensor([[1], [2], [3]])                       

In [3]: x                                                       
Out[3]: 
tensor([[1.],
        [2.],
        [3.]])

In [4]: x1 = [x*2 for i in range(1, 4)]                         

In [5]: x1                                                      
Out[5]: 
[tensor([[2.],
         [4.],
         [6.]]),
 tensor([[2.],
         [4.],
         [6.]]),
 tensor([[2.],
         [4.],
         [6.]])]

In [6]: xx1 = torch.cat((x1), 1)                                

In [7]: xx1                                                     
Out[7]: 
tensor([[2., 2., 2.],
        [4., 4., 4.],
        [6., 6., 6.]])

In [8]: xx1_1 = torch.cat(x1, 0)                               

In [9]: xx1_1                                                  
Out[9]: 
tensor([[2.],
        [4.],
        [6.],
        [2.],
        [4.],
        [6.],
        [2.],
        [4.],
        [6.]])
```

(2) **torch.stack()**

参考：[https://www.cnblogs.com/yifdu25/p/9399047.html](https://www.cnblogs.com/yifdu25/p/9399047.html)；[https://blog.csdn.net/Teeyohuang/article/details/80362756](https://blog.csdn.net/Teeyohuang/article/details/80362756)

`torch.stack()`与`torch.cat()`的区别在于：

- `cat`对数据沿某一维度进行拼接，`cat`后数据的总维度数不变；
- `stack`为增加新的维度进行堆叠。

如对两个1*2维的tensor在第0个维度上`stack`，则会变成2\*2\*2的tensor；在第1个维度上`stack`，则会变成1\*2\*2的tensor。

```python
In [2]: a = torch.rand(1, 2)                                    

In [3]: a                                                       
Out[3]: tensor([[0.5289, 0.9828]])

In [4]: b = torch.rand((1, 2))                                  

In [5]: b                                                       
Out[5]: tensor([[0.1610, 0.5676]])

In [6]: c = torch.stack((a, b), 0)                              

In [7]: c                                                       
Out[7]: 
tensor([[[0.5289, 0.9828]],

        [[0.1610, 0.5676]]])
```

对于高维度张量：

```python
In [2]: a = torch.randn((1, 2, 3, 4))                           

In [3]: a                                                       
Out[3]: 
tensor([[[[ 0.3603, -0.4322, -0.3183, -0.9004],
          [-0.5849,  1.2128, -1.0935,  0.4143],
          [ 1.4446,  1.3867,  1.8066, -2.2650]],

         [[ 2.2352, -1.2699, -0.5572, -1.8167],
          [-0.5426, -0.1514,  0.1314, -1.2296],
          [ 0.0166,  0.6789, -0.6616, -0.6927]]]])

In [4]: b = torch.stack((a, a), 0)                              

In [5]: b                                                       
Out[5]: 
tensor([[[[[ 0.3603, -0.4322, -0.3183, -0.9004],
           [-0.5849,  1.2128, -1.0935,  0.4143],
           [ 1.4446,  1.3867,  1.8066, -2.2650]],

          [[ 2.2352, -1.2699, -0.5572, -1.8167],
           [-0.5426, -0.1514,  0.1314, -1.2296],
           [ 0.0166,  0.6789, -0.6616, -0.6927]]]],



        [[[[ 0.3603, -0.4322, -0.3183, -0.9004],
           [-0.5849,  1.2128, -1.0935,  0.4143],
           [ 1.4446,  1.3867,  1.8066, -2.2650]],

          [[ 2.2352, -1.2699, -0.5572, -1.8167],
           [-0.5426, -0.1514,  0.1314, -1.2296],
           [ 0.0166,  0.6789, -0.6616, -0.6927]]]]])

In [6]: a.shape, b.shape                                        
Out[6]: (torch.Size([1, 2, 3, 4]), torch.Size([2, 1, 2, 3, 4]))

In [7]: c = torch.stack((a, a), 1)                              

In [8]: c                                                       
Out[8]: 
tensor([[[[[ 0.3603, -0.4322, -0.3183, -0.9004],
           [-0.5849,  1.2128, -1.0935,  0.4143],
           [ 1.4446,  1.3867,  1.8066, -2.2650]],

          [[ 2.2352, -1.2699, -0.5572, -1.8167],
           [-0.5426, -0.1514,  0.1314, -1.2296],
           [ 0.0166,  0.6789, -0.6616, -0.6927]]],


         [[[ 0.3603, -0.4322, -0.3183, -0.9004],
           [-0.5849,  1.2128, -1.0935,  0.4143],
           [ 1.4446,  1.3867,  1.8066, -2.2650]],

          [[ 2.2352, -1.2699, -0.5572, -1.8167],
           [-0.5426, -0.1514,  0.1314, -1.2296],
           [ 0.0166,  0.6789, -0.6616, -0.6927]]]]])

In [9]: a.shape, c.shape                                        
Out[9]: (torch.Size([1, 2, 3, 4]), torch.Size([1, 2, 2, 3, 4]))

In [10]: d = torch.stack((a, a), 3)                             

In [11]: a.shape, d.shape                                       
Out[11]: (torch.Size([1, 2, 3, 4]), torch.Size([1, 2, 3, 2, 4]))
```

根据高维度张量的`stack`可知，dim=0即在第0个维度前增加一个维度，dim=1即在第1个维度之前增加一个维度，...，依此类推。

(3) **torch.gather()**

参考：[https://blog.csdn.net/edogawachia/article/details/80515038](https://blog.csdn.net/edogawachia/article/details/80515038)；https://blog.csdn.net/Lucky_Rocks/article/details/79676095。

`torch.gather(input, dim, index)`表示沿给定的dim，将输入索引张量index指定位置的值进行聚合。

```python
In [18]: t = torch.Tensor([[1,2,3],[4,5,6]])                    

In [19]: t                                                      
Out[19]: 
tensor([[1., 2., 3.],
        [4., 5., 6.]])

In [20]: torch.gather(t, 1, torch.LongTensor([[1,0],[0,0]]))    
Out[20]: 
tensor([[2., 1.],
        [4., 4.]])
```

根据以上示例代码理解此函数：

* `dim=1`按列取；
* 第一组index为[1, 0]，第1列（实际的第二列）为2，第0列为1，所以第一行为[2, 1]；
* 同理，第二组index为[0, 0]，第0列为4，第0列为4，所以第二行为[4, 4]。
* 所以，最终结果为`[[2., 1.], [4., 4.]]`。

###### 4.1.3 slicing

(1) **torch.split()**

`torch.split(tensor, split_size, dim=0)`将输入张量分割为相等形状的块。

`split_size`表示需要切分的大小，可以为**`int`**和**`list`**两种类型。

***当`split_size`为`int`整数时***：

如果沿指定维的张量形状大小不能被split_size整分，则最后一个分块会小于其它分块。

```python
In [2]: x = torch.randn(3, 10, 6)                               

In [3]: x                                                       
Out[3]: 
tensor([[[-0.8775, -0.4960,  1.2960, -0.7834,  1.1593,  0.8485],
         [-1.3548, -1.0891,  0.3011,  0.5822,  0.1337, -0.9977],
         [-0.6412, -0.3102, -0.1618,  1.1270, -0.9163,  0.7583],
         [ 0.9933,  0.9254,  1.6719, -0.2064,  0.1526, -0.5263],
         [ 0.7706, -0.9688,  0.8912,  1.9223,  1.9731, -0.6831],
         [-0.0069,  0.2784,  1.2610, -0.3232, -1.1976, -0.1344],
         [ 0.0777,  0.7018, -0.5435, -0.8737, -0.4440,  1.0564],
         [ 0.1293, -0.8249,  0.7455,  1.2296,  0.4821,  1.3589],
         [ 0.3864, -1.4730,  0.6603,  0.5465,  0.1333, -0.7331],
         [ 1.7947, -0.5429, -1.0085,  0.6933,  0.6731, -1.2255]],

        [[ 0.0998, -0.0930, -0.4057,  0.2446,  0.7161,  0.3036],
         [-1.9477,  2.1896,  0.2929,  0.5595,  1.5185,  1.1661],
         [ 0.1391,  0.2492, -0.0041,  0.4088,  0.9216,  0.7747],
         [ 0.5324,  0.8214, -1.0135,  0.2731, -0.2142, -0.0330],
         [-0.7933,  1.4916,  0.0318,  0.5016, -0.0532, -1.2558],
         [-0.0261, -0.6143,  0.0034, -2.4476,  0.5431,  0.9995],
         [-1.4268,  1.0937, -1.7195,  0.9329,  2.3844,  0.0674],
         [ 1.2903,  1.1133, -0.9451, -0.7562, -0.8685, -0.6552],
         [ 0.7247, -2.1444, -0.0583,  0.0145,  0.6785, -0.3694],
         [ 1.3159,  0.1323, -1.5535,  1.4214, -0.7329, -0.6577]],

        [[ 0.8594, -0.7541,  0.4966, -0.2951, -0.7567,  1.3233],
         [-0.1438, -1.6067,  0.0128,  0.6936,  0.3204, -1.0199],
         [-0.4498,  1.3496,  0.7266,  0.3077,  0.1027,  0.2071],
         [ 0.8560,  0.6274,  1.2768, -0.9968,  0.6768, -0.5088],
         [ 0.2686, -0.3681, -1.1322,  0.9814,  0.9188, -0.1681],
         [-0.6616, -0.7909, -0.0702, -0.4303,  2.8490,  0.7392],
         [ 0.7906,  0.3060, -0.2649, -0.2921,  0.4829, -0.1724],
         [-0.1080,  0.9259,  0.0673, -0.4310,  0.1326,  0.2177],
         [ 0.8790,  1.0464,  1.4730, -1.7451,  1.1794, -0.0900],
         [-0.3713,  0.9280,  0.5084, -0.1260, -0.5660, -2.3400]]])

In [4]: a, b, c = x.split(1, 0)                                 

In [5]: a                                                       
Out[5]: 
tensor([[[-0.8775, -0.4960,  1.2960, -0.7834,  1.1593,  0.8485],
         [-1.3548, -1.0891,  0.3011,  0.5822,  0.1337, -0.9977],
         [-0.6412, -0.3102, -0.1618,  1.1270, -0.9163,  0.7583],
         [ 0.9933,  0.9254,  1.6719, -0.2064,  0.1526, -0.5263],
         [ 0.7706, -0.9688,  0.8912,  1.9223,  1.9731, -0.6831],
         [-0.0069,  0.2784,  1.2610, -0.3232, -1.1976, -0.1344],
         [ 0.0777,  0.7018, -0.5435, -0.8737, -0.4440,  1.0564],
         [ 0.1293, -0.8249,  0.7455,  1.2296,  0.4821,  1.3589],
         [ 0.3864, -1.4730,  0.6603,  0.5465,  0.1333, -0.7331],
         [ 1.7947, -0.5429, -1.0085,  0.6933,  0.6731, -1.2255]]])

In [6]: b                                                       
Out[6]: 
tensor([[[ 0.0998, -0.0930, -0.4057,  0.2446,  0.7161,  0.3036],
         [-1.9477,  2.1896,  0.2929,  0.5595,  1.5185,  1.1661],
         [ 0.1391,  0.2492, -0.0041,  0.4088,  0.9216,  0.7747],
         [ 0.5324,  0.8214, -1.0135,  0.2731, -0.2142, -0.0330],
         [-0.7933,  1.4916,  0.0318,  0.5016, -0.0532, -1.2558],
         [-0.0261, -0.6143,  0.0034, -2.4476,  0.5431,  0.9995],
         [-1.4268,  1.0937, -1.7195,  0.9329,  2.3844,  0.0674],
         [ 1.2903,  1.1133, -0.9451, -0.7562, -0.8685, -0.6552],
         [ 0.7247, -2.1444, -0.0583,  0.0145,  0.6785, -0.3694],
         [ 1.3159,  0.1323, -1.5535,  1.4214, -0.7329, -0.6577]]])

In [7]: c                                                       
Out[7]: 
tensor([[[ 0.8594, -0.7541,  0.4966, -0.2951, -0.7567,  1.3233],
         [-0.1438, -1.6067,  0.0128,  0.6936,  0.3204, -1.0199],
         [-0.4498,  1.3496,  0.7266,  0.3077,  0.1027,  0.2071],
         [ 0.8560,  0.6274,  1.2768, -0.9968,  0.6768, -0.5088],
         [ 0.2686, -0.3681, -1.1322,  0.9814,  0.9188, -0.1681],
         [-0.6616, -0.7909, -0.0702, -0.4303,  2.8490,  0.7392],
         [ 0.7906,  0.3060, -0.2649, -0.2921,  0.4829, -0.1724],
         [-0.1080,  0.9259,  0.0673, -0.4310,  0.1326,  0.2177],
         [ 0.8790,  1.0464,  1.4730, -1.7451,  1.1794, -0.0900],
         [-0.3713,  0.9280,  0.5084, -0.1260, -0.5660, -2.3400]]])

In [8]: d, e = x.split(2, 0)                                    

In [9]: d                                                       
Out[9]: 
tensor([[[-0.8775, -0.4960,  1.2960, -0.7834,  1.1593,  0.8485],
         [-1.3548, -1.0891,  0.3011,  0.5822,  0.1337, -0.9977],
         [-0.6412, -0.3102, -0.1618,  1.1270, -0.9163,  0.7583],
         [ 0.9933,  0.9254,  1.6719, -0.2064,  0.1526, -0.5263],
         [ 0.7706, -0.9688,  0.8912,  1.9223,  1.9731, -0.6831],
         [-0.0069,  0.2784,  1.2610, -0.3232, -1.1976, -0.1344],
         [ 0.0777,  0.7018, -0.5435, -0.8737, -0.4440,  1.0564],
         [ 0.1293, -0.8249,  0.7455,  1.2296,  0.4821,  1.3589],
         [ 0.3864, -1.4730,  0.6603,  0.5465,  0.1333, -0.7331],
         [ 1.7947, -0.5429, -1.0085,  0.6933,  0.6731, -1.2255]],

        [[ 0.0998, -0.0930, -0.4057,  0.2446,  0.7161,  0.3036],
         [-1.9477,  2.1896,  0.2929,  0.5595,  1.5185,  1.1661],
         [ 0.1391,  0.2492, -0.0041,  0.4088,  0.9216,  0.7747],
         [ 0.5324,  0.8214, -1.0135,  0.2731, -0.2142, -0.0330],
         [-0.7933,  1.4916,  0.0318,  0.5016, -0.0532, -1.2558],
         [-0.0261, -0.6143,  0.0034, -2.4476,  0.5431,  0.9995],
         [-1.4268,  1.0937, -1.7195,  0.9329,  2.3844,  0.0674],
         [ 1.2903,  1.1133, -0.9451, -0.7562, -0.8685, -0.6552],
         [ 0.7247, -2.1444, -0.0583,  0.0145,  0.6785, -0.3694],
         [ 1.3159,  0.1323, -1.5535,  1.4214, -0.7329, -0.6577]]])

In [10]: e                                                      
Out[10]: 
tensor([[[ 0.8594, -0.7541,  0.4966, -0.2951, -0.7567,  1.3233],
         [-0.1438, -1.6067,  0.0128,  0.6936,  0.3204, -1.0199],
         [-0.4498,  1.3496,  0.7266,  0.3077,  0.1027,  0.2071],
         [ 0.8560,  0.6274,  1.2768, -0.9968,  0.6768, -0.5088],
         [ 0.2686, -0.3681, -1.1322,  0.9814,  0.9188, -0.1681],
         [-0.6616, -0.7909, -0.0702, -0.4303,  2.8490,  0.7392],
         [ 0.7906,  0.3060, -0.2649, -0.2921,  0.4829, -0.1724],
         [-0.1080,  0.9259,  0.0673, -0.4310,  0.1326,  0.2177],
         [ 0.8790,  1.0464,  1.4730, -1.7451,  1.1794, -0.0900],
         [-0.3713,  0.9280,  0.5084, -0.1260, -0.5660, -2.3400]]])

In [11]: a.dtype, b.dtype, c.dtype, d.dtype, e.dtype            
Out[11]: (torch.float32, torch.float32, torch.float32, torch.float32, torch.float32)
  
In [12]: a.shape, b.shape, c.shape, d.shape, e.shape            
Out[12]: 
(torch.Size([1, 10, 6]),
 torch.Size([1, 10, 6]),
 torch.Size([1, 10, 6]),
 torch.Size([2, 10, 6]),
 torch.Size([1, 10, 6]))
```

逐条看各个命令：

```python
x = torch.randn(3, 10, 6) # 生成一个(3, 10, 6)的x张量
a, b, c = x.split(1, 0)   # 基于第0维，把x分为3个张量，于是每个张量shape俱为(1, 10, 6)
d, e = x.split(2, 0)      # 基于第0维，把x分为2个张量，第一个张量的shape为(2, 10, 6)，剩下的张量split_size不足2了，所以第二个张量的shape为(1, 10, 6)
```

***当`split_size`为`list`列表时***：

参考：[https://www.codetd.com/article/8938026](https://www.codetd.com/article/8938026)

```python
In [2]: x = torch.rand(3, 8, 6)                                 

In [3]: xa, xb, xc = torch.split(x, [2, 1, 5], dim=1)           

In [4]: xa.shape, xb.shape, xc.shape                            
Out[4]: (torch.Size([3, 2, 6]), torch.Size([3, 1, 6]), torch.Size([3, 5, 6]))
```

如上实例中所示，张量x基于第1维（实际的第**二**维）被分为`[2, 1, 5]`三个张量，此例中，x的第二维为8，所以分成的三个张量加和也必须等于8，即`2+1+5=8`。否则，会报错。

```python
In [5]: xi = torch.split(x, [2, 1, 1], dim=1)                                                                                     
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-5-5cd84dcf8582> in <module>
----> 1 xi = torch.split(x, [2, 1, 1], dim=1)

/usr/local/Caskroom/miniconda/base/lib/python3.7/site-packages/torch/functional.py in split(tensor, split_size_or_sections, dim)
     85     # split_size_or_sections. The branching code is in tensor.py, which we
     86     # call here.
---> 87     return tensor.split(split_size_or_sections, dim)
     88 
     89 # equivalent to itertools.product(indices)

/usr/local/Caskroom/miniconda/base/lib/python3.7/site-packages/torch/tensor.py in split(self, split_size, dim)
    384                 return super(Tensor, self).split_with_sizes(split_size, dim)
    385         else:
--> 386             return super(Tensor, self).split_with_sizes(split_size, dim)
    387 
    388     def unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None):

RuntimeError: split_with_sizes expects split_sizes to sum exactly to 8 (input tensor's size at dimension 1), but got split_sizes=[2, 1, 1]
```

报错信息：`RuntimeError: split_with_sizes expects split_sizes to sum exactly to 8 (input tensor's size at dimension 1), but got split_sizes=[2, 1, 1]`，要求分割`list`各值的加和与张量相应维度的值必须相等。

(2) **torch.chunk()**

参考：[https://www.codetd.com/article/8938026](https://www.codetd.com/article/8938026)

`torch.chunk(input, chunks)`基本使用和`torch.split()`相同。`input`为需要切分的tensor，`chunks`(int型)为需要切分后的块大小，`dim`为切分的维度。

区别：

* `chunks`只能是`int`型，而`torch.split`中`split_size`可以是list；
* `chunks`在块值小于该维度值时，会将块按照维度切分成1的结构。而split则会返回原来的张量。

```python
In [2]: x = torch.rand(2, 4, 6)                                 

In [3]: y = torch.chunk(x, 5, dim=1)                            

In [6]: for i in y: 
   ...:     print(i.size()) 
   ...:                                                         
torch.Size([2, 1, 6])
torch.Size([2, 1, 6])
torch.Size([2, 1, 6])
torch.Size([2, 1, 6])

In [7]: x.size()                                                
Out[7]: torch.Size([2, 4, 6])

In [8]: yy = torch.split(x, 5, dim=1) 
  
In [11]: for i in yy: 
    ...:     print(i.size()) 
    ...:                                                                                                                          
torch.Size([2, 4, 6])
```

###### 4.1.4 mutation

(1) **torch.transpose()**

参考：[https://www.cnblogs.com/yifdu25/p/9399047.html](https://www.cnblogs.com/yifdu25/p/9399047.html)

交换维度。

```python
In [2]: x = torch.randn(2, 3)                                   

In [3]: x                                                       
Out[3]: 
tensor([[-0.2395, -1.3414, -0.7900],
        [ 0.0117,  0.5133, -0.1420]])

In [4]: x.transpose(0, 1)                                       
Out[4]: 
tensor([[-0.2395,  0.0117],
        [-1.3414,  0.5133],
        [-0.7900, -0.1420]])
```

(2) **torch.t()**

(3) **torch.squeeze()** & **torch.unsqueeze()**

参考:

[https://www.cnblogs.com/yifdu25/p/9399047.html](https://www.cnblogs.com/yifdu25/p/9399047.html)；

[https://blog.csdn.net/xiexu911/article/details/80820028](https://blog.csdn.net/xiexu911/article/details/80820028)

squeeze和unsqueeze的用法主要是对数据的维度进行压缩或者解压。（这句话一点营养都没有🤭🤭😬)

函数功能：去掉size为1的维度，包括行和列。**当维度大于等于2时，squeeze()无作用**，**当维度大于等于2时，squeeze()无作用**。

其中，`squeeze(0)`代表第一维度值为1则去除第一维度，`squeeze(1)`代表若第二维度为1则去除第二维度。

用实例学习最靠谱：

```python
In [2]: x = torch.randn(1, 3)                                                                                                     
In [3]: x                                                                                                                         
Out[3]: tensor([[-0.0217,  1.5953,  0.5264]])

In [4]: xs = x.squeeze(0)                                                                                                         
In [5]: xs                                                                                                                        
Out[5]: tensor([-0.0217,  1.5953,  0.5264])

In [6]: x.size(), xs.size()                                                                                                       
Out[6]: (torch.Size([1, 3]), torch.Size([3]))
```

上述实例去除了第一维度，不太明显，继续：

```python
In [2]: x = torch.randn(3, 1)                                   

In [3]: x                                                       
Out[3]: 
tensor([[-0.1502],
        [ 0.0505],
        [ 0.1177]])

In [4]: xx = x.squeeze(1)                                       

In [5]: xx                                                      
Out[5]: tensor([-0.1502,  0.0505,  0.1177])

In [6]: x.size(), xx.size()                                     
Out[6]: (torch.Size([3, 1]), torch.Size([3]))
```

第二维度被去掉了，列变成行了，这个用得比较多。

`unqueeze`把上述操作恢复回来：

```python
In [9]: xx.unsqueeze(1)                                         
Out[9]: 
tensor([[-0.1502],
        [ 0.0505],
        [ 0.1177]])
        
In [10]: xx.unsqueeze(0)                                        
Out[10]: tensor([[-0.1502,  0.0505,  0.1177]])
```

自己体会吧，有点表达不出来了🥵。









(4) **torch.reshape()**

(5) **torch.unbind()**

(6) **torch.where()**

(7) **torch.nonzero()**





#### References

* [pytorch入坑一: Tensor及其基本操作](https://zhuanlan.zhihu.com/p/36233589)
* [Pytorch中.new()的作用详解](https://www.jb51.net/article/180679.htm)
* [浅谈pytorch和Numpy的区别以及相互转换方法](https://www.jb51.net/article/144520.htm)
* [**torch.eye**](https://www.cnblogs.com/btschang/p/10300727.html)
* [PyTorch的学习笔记](https://www.cnblogs.com/gato-chat/p/9064974.html)
* [pytorch.range() 和 pytorch.arange() 的区别](https://blog.csdn.net/m0_37586991/article/details/88830026)
* [PyTorch中linspace的详细用法](https://blog.csdn.net/york1996/article/details/81671128)
* [torch.normal()](https://blog.csdn.net/sxs11/article/details/81775715)
* [torch.randn和torch.rand有什么区别](https://blog.csdn.net/wangwangstone/article/details/89815661)
* [randperm函数](https://www.cnblogs.com/ymjyqsx/p/6200692.html)
* [Pytorch中index_select() 函数的实现理解](https://www.jb51.net/article/174524.htm)
* [torch.masked_select 掩码](https://blog.csdn.net/q511951451/article/details/81611903)
* [Pytorch mask_select 函数](https://blog.csdn.net/SoftPoeter/article/details/81667810)
* [Pytorch中的torch.cat()函数](https://www.cnblogs.com/JeasonIsCoding/p/10162356.html)
* [torch.stack()的使用](https://blog.csdn.net/Teeyohuang/article/details/80362756)
* [pytorch中的cat、stack、tranpose、permute、unsqeeze](https://www.cnblogs.com/yifdu25/p/9399047.html)
* [pytorch之torch.gather方法](https://blog.csdn.net/edogawachia/article/details/80515038)
* [pytorch中的gather函数](https://blog.csdn.net/edogawachia/article/details/80515038)
* [torch.split() 与 torch.chunk()](https://www.codetd.com/article/8938026)
* [pytorch学习 中 torch.squeeze() 和torch.unsqueeze()的用法](https://blog.csdn.net/xiexu911/article/details/80820028)

