---
title: 常用激活函数
author: 赵旭山
tags: 随笔
---

#### 1. 什么是激活函数

简而言之，即为线性组合的$ w \cdot x + b $的`Perceptrons`求和后，再叠加一个非线性操作（下图中的`step function`，使其可以处理非线性问题。

> “激励函数拿出自己最擅长的”掰弯利器”, 套在了原函数上 用力一扭, 原来的$ w \cdot x $结果就被扭弯了.”

![](/assets/images/activationFunction202003121121.jpg)

#### 2. 常用的激活函数

##### 2.1 Tanh函数

$ tanh(x) $为双曲正切函数：

$$ tanh(x) = \frac{sinh(x)}{cosh(x)} = \frac{e^x-e^{-x}}{e^x+e^{-x}} $$

$ tanh(x) $函数与$ sigmoid $函数有如下对应关系：

$$ tanh(x) = 2 \cdot sigmoid(2 \cdot x) - 1 $$

绘图如下，双曲正切函数$ tanh(x) $将$ [-\infty, \infty] $映射为$ [-1, 1] $ 。

![](/assets/images/tanh202003121153.png)

* **优点**：全程可导；输出区间为-1到1；解决了zero-centered的输出问题；
* **缺点**：存在梯度消失和爆炸的问题。

##### 2.2 Sigmoid函数

Sigmoid函数是深度学习领域开始时使用频率最高的激活函数。公式如下：

$$ \sigma(x) = \frac{1}{1+e^{-x}} $$

绘图如下：

![](/assets/images/sigmoid202003121212.png)

* **优点**：便于求导；将输出值压缩到0~1范围之内；

* **缺点**：

  * 容易出现梯度消失

    > “优化神经网络的方法是Back Propagation，即导数的后向传递：先计算输出层对应的loss，然后将loss以导数的形式不断向上一层网络传递，修正相应的参数，达到降低loss的目的。但当x较大或较小时，导数接近0；并且Sigmoid函数导数的最大值是0.25，导数在每一层至少会被压缩为原来的1/4。正是因为这两个原因，从输出层不断向输入层反向传播训练时，导数很容易逐渐变为0，使得权重和偏差参数无法被更新，导致神经网络无法被优化。”

  * 输出不是zero-centered

    > “Sigmoid函数的输出值恒大于0，假设后层的输入都是非0的信号，在反向传播过程中，weight要么是都往正方向更新，要么都往负方向更新，按照图中所示的阶梯式更新，并非好的优化路径，计算量较大，模型收敛的速度减慢。”

  * 幂运算相对耗时

    > “相对于前两项，这其实并不是一个大问题，我们目前是具备相应计算能力的，但之后我们会看到，在ReLU函数中，计算机需要做的仅仅是一个thresholding，相对于幂运算来讲会快很多。”

#### 3. ReLU函数

全称Rectified Linear Unit（ReLU），其实就是一个取最大值的函数。

$$ ReLU = max(0, x) $$

![](/assets/images/relu202003121812.jpg)

> “注意这并不是全区间可导的，但是我们可以取次梯度(subgradient)。”

**优点**：

> * 解决了梯度消失的问题 (在正区间)
>
> * 计算速度非常快，只需要判断输入是否大于0
> * 收敛速度远快于sigmoid和tanh
> * Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生

**缺点**：

> * 输出不是zero-centered
>
> * Dead ReLU Problem
>
>   ---
>
>   Dead ReLU Problem指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) 学习速率太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将学习速率设置太大或使用adagrad等自动调节学习速率的算法。
>
>   ---


#### References

* [常用激活函数比较](https://www.jianshu.com/p/22d9720dbf1a)

* [机器学习笔记-神经网络中激活函数（activation function）对比--Sigmoid、ReLu，tanh](https://blog.csdn.net/lilu916/article/details/77822309)

* [激励函数 (Activation)](https://morvanzhou.github.io/tutorials/machine-learning/torch/2-03-activation/)

* [激励函数 (Activation Function)](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-04-activation-function/)