---
title: 常用激活函数
author: 赵旭山
tags: 随笔
---

#### 1. 什么是激活函数

简而言之，即为线性组合的$ w \cdot x + b $的`Perceptrons`求和后，再叠加一个非线性操作（下图中的`step function`，使其可以处理非线性问题。

> “激励函数拿出自己最擅长的”掰弯利器”, 套在了原函数上 用力一扭, 原来的$ w \cdot x $结果就被扭弯了.”

![](/assets/images/activationFunction202003121121.jpg)

#### 2. 常用的激活函数

##### 2.1 Tanh函数

$ tanh(x) $为双曲正切函数：

$$ tanh(x) = \frac{sinh(x)}{cosh(x)} = \frac{e^x-e^{-x}}{e^x+e^{-x}} $$

$ tanh(x) $函数与$ sigmoid $函数有如下对应关系：

$$ tanh(x) = 2 \cdot sigmoid(2 \cdot x) - 1 $$

绘图如下，双曲正切函数$ tanh(x) $将$ [-\infty, \infty] $映射为$ [-1, 1] $ 。

![](/assets/images/tanh202003121153.png)

* **优点**：全程可导；输出区间为-1到1；解决了zero-centered的输出问题；
* **缺点**：存在梯度消失和爆炸的问题。











#### References

* [常用激活函数比较](https://www.jianshu.com/p/22d9720dbf1a)

* [机器学习笔记-神经网络中激活函数（activation function）对比--Sigmoid、ReLu，tanh](https://blog.csdn.net/lilu916/article/details/77822309)

* [激励函数 (Activation)](https://morvanzhou.github.io/tutorials/machine-learning/torch/2-03-activation/)

* [激励函数 (Activation Function)](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-04-activation-function/)